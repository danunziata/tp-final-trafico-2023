{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"Trabajo final de la materia TR\u00c1FICO 2023"},{"location":"#integrantes-del-equipo","title":"Integrantes del equipo:","text":"<ol> <li>Buten Andr\u00e9s Benjam\u00edn</li> <li>Bustos Juan Cruz</li> <li>Demaio Ignacio</li> <li>Haponiuk Kevin Joel</li> <li>Lopez Alejo</li> <li>Olivo Alejo Gonzalo</li> </ol>"},{"location":"#integrantes-del-grupo-2","title":"Integrantes del Grupo 2:","text":"<ul> <li>Nu\u00f1ez Damian</li> <li>Martinelli Axel</li> <li>Picco Francisco</li> <li>Tizzian Ramiro</li> </ul>"},{"location":"#integrantes-del-grupo-3","title":"Integrantes del Grupo 3:","text":"<ul> <li>Lahiton Milena</li> <li>Mancini Matias</li> <li>Orellana Sebastian -Torletti Lautaro</li> </ul> <p>Agradecemos a los profesores por su orientaci\u00f3n y apoyo durante el desarrollo de este proyecto.</p>"},{"location":"2-resumen/","title":"Resumen","text":"<p>El trabajo practico final tiene como objetivo principal realizar un an\u00e1lisis exhaustivo del desempe\u00f1o de un modelo de escalado din\u00e1mico para servicios en la nube basados en contenedores, seg\u00fan lo propuesto en el paper de referencia \"Dynamic Scalability Model for Containerized Cloud Services\" (disponible en https://www.researchgate.net/publication/343586808_Dynamic_Scalability_Model_for_Containerized_Cloud_Services). Para lograr esto, se llevaron a cabo dos objetivos espec\u00edficos.</p> <p>El objetivo inicial del trabajo involucr\u00f3 la implementaci\u00f3n en Python de los modelos propuestos en el paper de referencia, seguido de la representaci\u00f3n gr\u00e1fica y explicaci\u00f3n detallada de los resultados obtenidos, destacando la necesidad de una comprensi\u00f3n s\u00f3lida en la teor\u00eda. Ahora en el segundo objetivo, el enfoque se desplaza hacia el despliegue efectivo de estos modelos en un entorno realista y controlado mediante un Homelab para emular y evaluar el desempe\u00f1o del modelo de escalado din\u00e1mico para servicios en la nube basados en contenedores.</p> <p>Esto incluir\u00e1 la implementaci\u00f3n de un escenario utilizando el framework FastApi para comprender su funcionamiento y su integraci\u00f3n con servicios en la nube basados en contenedores. Adem\u00e1s, se desplegar\u00e1n las soluci\u00f3nes propuesta en contenedores mediante Docker, evaluando su rendimiento en este entorno. Se explorar\u00e1 tambi\u00e9n la implementaci\u00f3n del escenario con balanceador de carga y se desplegar\u00e1 la soluci\u00f3n en un entorno clusterizado utilizando Minikube, permitiendo as\u00ed el an\u00e1lisis del comportamiento del sistema en un entorno de m\u00faltiples nodos y la evaluaci\u00f3n de la escalabilidad del modelo implementado.</p> <p>Se establecer\u00e1n m\u00e9tricas y trazabilidad del sistema propuesto, definiendo indicadores clave de desempe\u00f1o y configurando herramientas de monitoreo para recopilar datos relevantes durante las pruebas. Se utilizar\u00e1n herramientas de prueba de carga como Locust para simular escenarios de alto tr\u00e1fico y evaluar el rendimiento del sistema, midiendo tiempos de respuesta, capacidad de escalado y estabilidad bajo carga.</p> <p>Los resultados obtenidos en las pruebas se evaluar\u00e1n y presentar\u00e1n, compar\u00e1ndolos con los objetivos planteados inicialmente. Se extraer\u00e1n conclusiones y se ofrecer\u00e1n recomendaciones basadas en los hallazgos obtenidos durante el an\u00e1lisis.</p>"},{"location":"3-intro/","title":"Introduccion","text":"<p>La r\u00e1pida evoluci\u00f3n de la computaci\u00f3n en la nube ha impulsado la necesidad de desarrollar modelos eficientes que permitan adaptarse din\u00e1micamente a la creciente demanda de servicios. En el contexto de esta transformaci\u00f3n, este trabajo se centra en la evaluaci\u00f3n pr\u00e1ctica de un modelo de escalado din\u00e1mico para servicios en la nube basados en contenedores, abordando la segunda fase del trabajo que busca no solo entender la teor\u00eda detr\u00e1s de estos modelos, sino tambi\u00e9n analizar su rendimiento en entornos simulados y controlados. En la continuaci\u00f3n de nuestro trabajo centrado en el an\u00e1lisis del desempe\u00f1o de un modelo de escalado din\u00e1mico para servicios en la nube basados en contenedores, nos adentramos en el segundo objetivo, donde nos enfocamos en la evaluaci\u00f3n pr\u00e1ctica y despliegue de los servicios en un entorno controlado.</p> <p>La metodolog\u00eda utilizada para el desarrollo del objetivo 2 consto en la formacion de 3 grupos especializados en aspectos claves de la implementacion, contribuyendo de manera significativa a la comprensi\u00f3n integral del sistema. La colaboraci\u00f3n de estos grupos se ha centrado en la investigaci\u00f3n y aplicaci\u00f3n pr\u00e1ctica de tres componentes esenciales del escenario propuesto: el load balancer, el generador de tr\u00e1fico, y la implementaci\u00f3n con Minikube. </p> <p>El primer grupo se ha dedicado a investigar y comprender como gestionar y generar el tr\u00e1fico en un entorno basado en contenedores. Este enfoque abarca desde la implementaci\u00f3n de escenarios con FastApi hasta el uso de herramientas como Locust y/o K6 para simular escenarios de alto tr\u00e1fico. El analisis detalladado de estas din\u00e1micas del tr\u00e1fico permitir\u00e1 una evaluaci\u00f3n precisa del rendimiento del sistema bajo condiciones diversas.</p> <p>El segundo grupo ha abordado la implementaci\u00f3n de un load balancer, explorando tecnolog\u00edas como Nginx, HAProxy y Traefik para asegurar una distribuci\u00f3n eficiente del tr\u00e1fico entre los nodos del sistema. Esta investigaci\u00f3n proporcionar\u00e1 una base crucial para entender c\u00f3mo la carga se distribuye de manera equitativa, mejorando as\u00ed la estabilidad y rendimiento del sistema.</p> <p>El tercer grupo ha explorado la implementaci\u00f3n con Minikube, utilizando Kubernetes como plataforma de orquestaci\u00f3n de contenedores, y empleando Minikube para gestionar los clusters locales, centr\u00e1ndose en la creaci\u00f3n de un entorno clusterizado. Este grupo ha investigado c\u00f3mo la soluci\u00f3n propuesta se comporta en un escenario de m\u00faltiples nodos, brindando comprenciones valiosas sobre la escalabilidad y comportamiento del modelo en un entorno m\u00e1s complejo.</p> <p>El resultado de la colaboraci\u00f3n entre estos tres grupos proporciono una evaluaci\u00f3n completa del modelo de escalado din\u00e1mico, ademas de proveer una gu\u00eda pr\u00e1ctica para implementaciones futuras de servicios en la nube basados en contenedores. Cada grupo aporto su experiencia para construir el panorama completo de la implementaci\u00f3n del escenario y rendimiento en su conjunto.</p>"},{"location":"5-cliente-servidor/","title":"Cliente - Servidor","text":"<p>El objetivo es desarrollar un modelo de colas M/M/1 donde el tiempo de interarribo y el largo de las tareas poseen distribucion exponencial. Se hizo uso de Locust como cliente generador de tr\u00e1fico, mientras que el servidor es una aplicaci\u00f3n creada con FastApi. Ver implementaci\u00f3n en README</p> <p>A continuaci\u00f3n se presentan los conceptos de Locust y FastApi y su utilizaci\u00f3n.</p>"},{"location":"5-cliente-servidor/#locust","title":"Locust","text":""},{"location":"5-cliente-servidor/#introduccion","title":"Introducci\u00f3n","text":"<p>Locust en Linux es una herramienta de c\u00f3digo abierto para realizar pruebas de carga y estr\u00e9s en aplicaciones web. Se utiliza para simular un gran n\u00famero de usuarios accediendo a una aplicaci\u00f3n web al mismo tiempo, lo que ayuda a identificar y solucionar problemas de rendimiento y escalabilidad.</p> <p>Locust es una herramienta de prueba de rendimiento escalable, programable y f\u00e1cil de usar. Algunas de sus caracter\u00edsticas son:</p> <ul> <li>Pruebas de Rendimiento: Locust permite medir el rendimiento de una aplicaci\u00f3n web al simular la carga que podr\u00eda experimentar en un entorno de producci\u00f3n. Esto ayuda a identificar cuellos de botella, problemas de escalabilidad y a optimizar el rendimiento.</li> <li>Identificaci\u00f3n de Problemas: Al simular el comportamiento de usuarios reales, Locust ayuda a identificar problemas como tiempos de respuesta lentos, errores de servidor, y otros problemas de rendimiento que podr\u00edan afectar la experiencia del usuario.</li> <li>Escalabilidad: Locust permite evaluar c\u00f3mo una aplicaci\u00f3n maneja un aumento en el n\u00famero de usuarios concurrentes. Esto es crucial para garantizar que la aplicaci\u00f3n pueda escalar de manera efectiva a medida que la demanden este trabajoa aumenta.</li> <li>Monitoreo en Tiempo Real: Proporciona m\u00e9tricas en tiempo real durante las pruebas, lo que permite a los desarrolladores y equipos de operaciones obtener informaci\u00f3n inmediata sobre el comportamiento y el rendimiento del sistema.</li> <li>Configuraci\u00f3n Flexible: Locust permite definir escenarios de prueba mediante c\u00f3digo Python, brindando flexibilidad para adaptarse a situaciones espec\u00edficas de uso. Esto incluye la capacidad de simular diferentes patrones de tr\u00e1fico y comportamientos de usuario.</li> <li>Generaci\u00f3n de Informes: Locust genera informes detallados que ayudan a interpretar los resultados de las pruebas. Esto facilita la identificaci\u00f3n de \u00e1reas de mejora y proporciona datos valiosos para la toma de decisiones.</li> </ul>"},{"location":"5-cliente-servidor/#utilizacion","title":"Utilizaci\u00f3n","text":"<p>Locust se utiliza para generar tr\u00e1fico externo con tasa de arribo de Poisson y tiempos de interarribo de paquetes con distribuci\u00f3n exponencial, por lo tanto depende de un solo par\u00e1metro que es el valor medio.</p> <p>Se hace uso de un paradigma cliente-servidor donde el cliente hace referencia a la generaci\u00f3n de tr\u00e1fico por parte de Locust, mientras que el servidor es Uvicorn.</p>"},{"location":"5-cliente-servidor/#instalacion","title":"Instalaci\u00f3n","text":"<p>Antes de instalar Locust, se necesita tener Python que, por lo general, est\u00e1 instalado en Linux, pero de lo contrario el comando es:</p> <pre><code>sudo apt-get install python3\n</code></pre> <p>En caso de no tener el gestor de paquetes de python</p> <pre><code>sudo apt install python3-pip\n</code></pre> <p>A continuaci\u00f3n se indica el siguiente comando utilizado para instalar locust</p> <pre><code>pip install locust\n</code></pre>"},{"location":"5-cliente-servidor/#implementacion","title":"Implementaci\u00f3n","text":"<p>Creando un archivo cliente Locust:</p> <pre><code>touch cliente.py\n</code></pre> <p>Para realizar el script del cliente, primero realizamos uno con distribuci\u00f3n exponencial de los tiempos de interarribo utilizando la librer\u00eda de \"time\" por lo que era s\u00edncrona y se deb\u00eda esperar la respuesta para enviar una nueva tarea. Se encuentra en el siguiente link de GitHub: cliente_exp_time.py</p> <pre><code>from locust import HttpUser, task, between\nimport time, random\nlambd=100\nclass HelloWorldUser(HttpUser):\n\n    @task\n    def hello_world(self):\n        a=random.expovariate(lambd)\n        time.sleep(a)\n        self.client.get(\"/\")\n</code></pre> <p>En primer lugar se importan m\u00f3dulos <code>from locust import HttpUser, task, between</code>.</p> <p>Luego, se define una variable llamada <code>lambd</code> con un valor de 100. Esta variable aparentemente representa la tasa de llegada (lambda) para la distribuci\u00f3n exponencial.</p> <p>Por otro lado, se define una clase <code>HelloWorldUser</code> que hereda de HttpUser. Esta clase representa un usuario virtual que realizar\u00e1 pruebas de carga en la aplicaci\u00f3n web.</p> <p><code>@task</code>: Decorador que define una tarea llamada hello_world. La tarea simula el comportamiento de un usuario que realiza una solicitud a la ra\u00edz (\"/\") de la aplicaci\u00f3n web.</p> <p>Con <code>a</code> se genera tiempo de espera utilizando la distribuci\u00f3n exponencial. La funci\u00f3n random.expovariate genera n\u00fameros distribuidos exponencialmente con una tasa dada por lambd.</p> <p>Espera Sincr\u00f3nica: <code>time.sleep(a)</code> Hace que el usuario espere durante el tiempo generado (a). Esto simula el tiempo que un usuario real podr\u00eda esperar entre solicitudes.</p> <p>Realizaci\u00f3n de Solicitud HTTP:<code>self.client.get(\"/\")</code>: Realiza una solicitud GET a la ruta \"/\" de la aplicaci\u00f3n web utilizando el cliente HTTP proporcionado por Locust.</p> <p>Por otro lado, tambi\u00e9n realizamos un programa que realiza solicitudes http con distribuci\u00f3n uniforme y asincrono. Se encuentra en el siguiente link de GitHub: cliente_unif_async.py</p> <pre><code>from locust import HttpUser, task, between\nimport random, asyncio\n#lambd=1000\nclass HelloWorldUser(HttpUser):\n\n    @task\n    def hello_world(self):\n        asyncio.sleep(0.01)\n        self.client.get(\"/\")\n</code></pre> <p>Este es similiar al anterior, con la diferencia que el segundo script utiliza <code>asyncio.sleep</code>, lo que indica que la espera se maneja de manera as\u00edncrona. Esto permite que otras tareas se ejecuten durante la espera, lo que puede ser \u00fatil en escenarios de carga donde se espera que m\u00faltiples usuarios realicen solicitudes simult\u00e1neamente.</p> <p>Por \u00faltimo, hicimos un script con un programa de python que no se ejecuta con el generador de tr\u00e1fico Locust. Cliente_Final</p> <pre><code>import aiohttp\nimport asyncio\nimport random\n\n# Lista para almacenar los tiempos de respuesta\nresponse_times = []\n\nasync def send_request(session, host, port, path, user_id):\n    url = f\"http://{host}:{port}{path}\"\n\n    # Medir el tiempo antes de enviar la solicitud\n    start_time = asyncio.get_event_loop().time()\n\n    # Iniciar la solicitud sin esperar la respuesta\n    async with session.get(url) as response:\n        #Para hacer que no espere la respuesta del servidor quitar el comentario en la siguiente linea y comentar la otra seccion\n        #-----pass\n\n        #La siguientes lineas hacen que el cliente espere la respuesta\n        data = await response.text()\n        #-----print(f\"Response from server: {data}\")\n\n\n    # Medir el tiempo despu\u00e9s de recibir la respuesta y almacenar el tiempo de respuesta\n    end_time = asyncio.get_event_loop().time()\n    response_time = end_time - start_time\n    response_times.append(response_time)\n    print(f\"response time  {response_time}\")\n\n    # Mostrar el tiempo promedio cada 100 paquetes\n    if len(response_times) % 100 == 0:\n        average_response_time = sum(response_times) / len(response_times)\n        print(f\"Avg. response time after {len(response_times)} packets: {average_response_time} seconds\")\n\nasync def generate_traffic(user_id, session, host, port, path, lambda_value):\n    while True:\n        # Esperar un tiempo seg\u00fan la distribuci\u00f3n exponencial antes de comenzar\n        inter_arrival_time = random.expovariate(lambda_value)\n        #-----print(f\"User {user_id}: Will wait for {inter_arrival_time} seconds before starting\")\n\n        await asyncio.sleep(inter_arrival_time)\n\n        # Ejecutar la solicitud\n        await send_request(session, host, port, path, user_id)\n\nasync def main():\n    host = \"192.168.1.109\"\n    port = 2023  # Reemplaza con el puerto correcto\n    path = \"/\" # Reemplaza con la ruta correcta\n\n    num_users = int(input(\"Ingrese la cantidad de usuarios: \"))\n    lambda_value = float(input(\"Ingrese el valor de lambda para la distribuci\u00f3n exponencial: \"))\n\n    async with aiohttp.ClientSession() as session:\n        user_tasks = [generate_traffic(user_id, session, host, port, path, lambda_value) for user_id in range(1, num_users + 1)]\n        await asyncio.gather(*user_tasks)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Este tercer programa de cliente es generador de tr\u00e1fico tambi\u00e9n con el objetivo de sustituir a Locust por varias cuestiones como aumento de RPS m\u00e1ximo. No entendimos bien el funcionamiento de Locust.</p> <p>Explicando el programa:</p> <ul> <li> <p>Solicitudes de Usuarios: Se simulan usuarios que env\u00edan solicitudes al servidor web. Cada usuario espera un tiempo aleatorio antes de enviar una solicitud, imitando la llegada no uniforme de usuarios.</p> </li> <li> <p>Tiempo de Respuesta: Se mide el tiempo que tarda el servidor en responder a cada solicitud. Los tiempos de respuesta se almacenan en una lista llamada response_times.</p> </li> <li> <p>C\u00e1lculo del Tiempo Promedio: Cada vez que se alcanza un m\u00faltiplo de 100 solicitudes, se calcula y muestra el tiempo promedio de respuesta hasta ese momento.</p> </li> <li> <p>Configuraci\u00f3n del Servidor: Se configura la direcci\u00f3n del servidor, el puerto y la ruta a la que se enviar\u00e1n las solicitudes.</p> </li> <li> <p>Configuraci\u00f3n de Usuarios y Ejecuci\u00f3n: El usuario ingresa la cantidad de usuarios y un par\u00e1metro llamado lambda que afecta la frecuencia de llegada de los usuarios. Se utilizan asyncio y aiohttp para manejar las operaciones as\u00edncronas y ejecutar las simulaciones de usuarios en paralelo.</p> </li> </ul>"},{"location":"5-cliente-servidor/#ejecucion","title":"Ejecuci\u00f3n","text":"<p>Para ejecutarlo debemos estar situados en el directorio donde se encuentre el archivo cliente.py. Una vez all\u00ed, implementamos el siguiente comando:</p> <pre><code>locust -f \u201cnombre del archivo.py\u201d\n</code></pre> <p>Una vez iniciado eso, ir a la direcci\u00f3n que te aparece, por ejemplo Starting web interface at http://0.0.0.0:8089 (accepting connections from all network interfaces)</p> <p>Cuando ingresamos a esa direcci\u00f3n, deber\u00edamos de ver la interfaz de locust donde podemos comenzar un nuevo test y debemos ingresar 3 par\u00e1metros:</p> <ul> <li>n\u00famero de usuarios: Es el n\u00famero m\u00e1ximo de usuarios al mismo tiempo en el sistema.</li> <li>Spawn rate: Cantidad de usuarios que aparecen por segundo (dado que el c\u00f3digo del cliente tiene una aparici\u00f3n exponencial hace que no sea de manera lineal)</li> <li>Host: debemos ingresar la ip y puerto del servidor (en este caso es http://192.168.1.199:2023). Si se realiza de manera local, dejar este campo vac\u00edo.</li> </ul> <p>Debido a los problemas que se han tenido con este generador de tr\u00e1fico, se ha utilizado como cliente y generador de tr\u00e1fico al archivo de python que se muestra en el siguiente enlace: Cliente_Final</p>"},{"location":"5-cliente-servidor/#fast-api","title":"Fast API","text":""},{"location":"5-cliente-servidor/#introduccion_1","title":"Introducci\u00f3n","text":"<p>FastAPI es un moderno marco de desarrollo web para la construcci\u00f3n de APIs (Interfaces de Programaci\u00f3n de Aplicaciones) con Python 3.7 o versiones posteriores. Fue creado por Sebasti\u00e1n Ram\u00edrez y se destaca por su rendimiento, facilidad de uso y generaci\u00f3n autom\u00e1tica de documentaci\u00f3n interactiva.</p> <p>Algunas de las caracter\u00edsticas clave de FastAPI incluyen:</p> <ul> <li>Rendimiento: FastAPI es conocido por ser extremadamente r\u00e1pido debido a su implementaci\u00f3n basada en el est\u00e1ndar Starlette y el uso de Pydantic para la validaci\u00f3n y la serializaci\u00f3n de datos.</li> <li>Tipado Est\u00e1tico: Hace un amplio uso de las anotaciones de tipo de Python para proporcionar un sistema de tipado est\u00e1tico que facilita la detecci\u00f3n temprana de errores y mejora la autocompletaci\u00f3n en entornos de desarrollo.</li> <li>Generaci\u00f3n Autom\u00e1tica de OpenAPI y Swagger: FastAPI genera autom\u00e1ticamente documentaci\u00f3n interactiva (Swagger) y una especificaci\u00f3n OpenAPI para tu API, lo que facilita el entendimiento y la prueba de tus endpoints.</li> <li>Sintaxis Declarativa: El dise\u00f1o de FastAPI permite una sintaxis clara y declarativa para definir rutas y modelos de datos, lo que simplifica el desarrollo de APIs de manera eficiente.</li> <li>Validaci\u00f3n de Datos Integrada: Utiliza Pydantic para realizar la validaci\u00f3n autom\u00e1tica de datos de entrada y salida, garantizando la consistencia y la integridad de los datos que entran y salen de la API.</li> <li>Soporte para WebSockets: FastAPI ofrece soporte integrado para el protocolo WebSocket, permitiendo la construcci\u00f3n de aplicaciones en tiempo real.</li> <li>Seguridad Integrada: Proporciona herramientas integradas para manejar la autenticaci\u00f3n y la autorizaci\u00f3n, incluyendo el uso de est\u00e1ndares como OAuth2 y JWT (JSON Web Tokens).</li> <li>Escalabilidad: Puede manejar de manera eficiente altas cargas de tr\u00e1fico y escalar para adaptarse a las demandas de aplicaciones de gran envergadura.</li> </ul>"},{"location":"5-cliente-servidor/#utilizacion_1","title":"Utilizaci\u00f3n","text":"<p>Fast API se utiliza para generar la aplicacion del servidor, donde nos va a devolver los tiempos de ejecucion del script de python + los tiempos de espera de la variable.</p>"},{"location":"5-cliente-servidor/#instalacion_1","title":"Instalaci\u00f3n","text":"<p>En primer lugar se debe instalar FastApi con el siguiente comando:</p> <pre><code>pip install FastAPI\n</code></pre> <p>Luego, se necesita el servidor Uvicorn, por lo que se debe implementar la siguiente linea en el terminal:</p> <pre><code>pip install uvicorn\n</code></pre>"},{"location":"5-cliente-servidor/#implementacion_1","title":"Implementaci\u00f3n","text":"<p>En cuanto al servidor, tambi\u00e9n hay tres programas. Hemos realizado algunas pruebas con un programa cuyo servidor tenia un valor constante de <code>sleep</code> en lugar de ser una variable aleatoria con distribuci\u00f3n exponencial. servidor_unif_async.py</p> <pre><code>from fastapi import FastAPI\n#import random\nimport asyncio\n\napp = FastAPI()\n#mu = 100    # 1 / media\n\n@app.get(\"/\")\nasync def root():\n    a = 0.01\n    asyncio.sleep(a)\n    return {1}\n</code></pre> <p>Este c\u00f3digo define una aplicaci\u00f3n FastAPI con una sola ruta (\"/\") que simula una peque\u00f1a espera antes de enviar una respuesta simple (un diccionario con el n\u00famero 1) al cliente que realiza la solicitud. Este tipo de espera puede ser \u00fatil para simular ciertos comportamientos as\u00edncronos en una aplicaci\u00f3n web, aunque en este caso, la espera es fija en 0.01 segundos.</p> <p>Tambi\u00e9n realizamos un script con distribuci\u00f3n exponencial as\u00edncrono. servidor_exp_async.py</p> <pre><code>from fastapi import FastAPI\nimport random, asyncio\napp = FastAPI()\nmu = 100\n\n@app.get(\"/\")\nasync def root():\n    a = random.expovariate(mu)\n    asyncio.sleep(a)\n    return {1}\n</code></pre> <p>Respecto del anterior, la principal diferencia entre ambos c\u00f3digos radica en la generaci\u00f3n del tiempo de espera. El segundo c\u00f3digo utiliza una distribuci\u00f3n exponencial para determinar din\u00e1micamente el tiempo de espera antes de responder, mientras que el primer c\u00f3digo utiliza un tiempo de espera fijo. Ambos c\u00f3digos simulan la espera asincr\u00f3nica antes de enviar una respuesta en una aplicaci\u00f3n web utilizando FastAPI.</p> <p>Por otro lado, tambi\u00e9n hemos realizado algunas pruebas con un programa cuyo servidor tenia una variable aleatoria con distribuci\u00f3n exponencial sincrona. servidor_exp_time.py</p> <pre><code>from fastapi import FastAPI\nimport random, time\napp = FastAPI()\nmu = 100\n\n@app.get(\"/\")\nasync def root():\n    a = random.expovariate(mu)\n    time.sleep(a)\n    return {1}\n</code></pre> <p>La diferencia clave entre este c\u00f3digo y el anterior es la elecci\u00f3n de la funci\u00f3n de espera (time.sleep en lugar de asyncio.sleep), lo que afecta el comportamiento de espera y la capacidad de la aplicaci\u00f3n para manejar m\u00faltiples solicitudes concurrentes de manera eficiente.</p>"},{"location":"5-cliente-servidor/#ejecucion_1","title":"Ejecuci\u00f3n","text":"<p>Una vez creado los programas tanto para el cliente como el servidor, para ejecutar es necesario utilizar el servidor Uvicorn para levantar la aplicaci\u00f3n creada con FastApi.</p> <p>Uvicorn es una implementaci\u00f3n de servidor web ASGI (Asynchronous Server Gateway Interface) para Python. ASGI es una especificaci\u00f3n que permite la creaci\u00f3n de aplicaciones web asincr\u00f3nicas en Python. Uvicorn es una implementaci\u00f3n de referencia para esta especificaci\u00f3n y est\u00e1 dise\u00f1ado para trabajar con frameworks web asincr\u00f3nicos como FastAPI.</p> <p>Algunas caracter\u00edsticas clave de Uvicorn incluyen:</p> <ul> <li>Asincron\u00eda: Uvicorn est\u00e1 dise\u00f1ado para manejar operaciones de entrada/salida de manera eficiente mediante el uso de corutinas y el bucle de eventos asyncio.</li> <li>Compatibilidad con ASGI: Al ser un servidor ASGI, Uvicorn puede trabajar con aplicaciones web que sigan la especificaci\u00f3n ASGI, permitiendo la construcci\u00f3n de aplicaciones web asincr\u00f3nicas y eficientes en Python.</li> <li>Rendimiento: Uvicorn se esfuerza por ofrecer un rendimiento elevado y es capaz de manejar un gran n\u00famero de conexiones concurrentes.</li> <li>Facilidad de Uso: Es f\u00e1cil de configurar y utilizar. Puede iniciarse directamente desde la l\u00ednea de comandos o integrarse en scripts de Python.</li> <li>Compatibilidad con FastAPI: Uvicorn es la opci\u00f3n recomendada para ejecutar aplicaciones creadas con FastAPI, un moderno framework web r\u00e1pido para Python.</li> </ul> <pre><code>#para este caso usamos servidor con distribuci\u00f3n expoencial sincrono.\nuvicorn servidor_exp_time:app --host 0.0.0.0 --port 8001 --reload\n</code></pre> <p>Aqu\u00ed, \"servidor\" es el nombre del archivo Python (sin la extensi\u00f3n .py) que contiene la aplicaci\u00f3n FastAPI, y app es el nombre de la instancia de la aplicaci\u00f3n dentro de ese archivo.</p> <p>La implementaci\u00f3n del servidor se realiz\u00f3 sobre una imagen de Docker, que es subida a DockerHub. De esta manera, todos pueden tener acceso y para funcionar es necesario modificar la linea de los deployments de Kubernetes que hace referencia a la imagen que selecciona para la creaci\u00f3n del contenedor y la linea de comando CMD que se visualiza en el mismo archivo.</p> <p>A continuaci\u00f3n se muestra uno de los deployments utilizados y comentamos las lineas que deben ser modificadas.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depokevina\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: php-kevin-a\n  template:\n    metadata:\n      labels:\n        role: php-kevin-a\n    spec:\n      nodeSelector:\n        kubernetes.io/hostname: minikube-m02\n      containers:\n        - name: php-kevin\n          image: bocha2002/servidor_exp_time:latest ##ESTA LINEA SE DEBE MODIFICAR DE ACUERDO AL SERVIDOR\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8000\n          command: [\n              \"/bin/sh\",\n              \"-c\",\n              \"uvicorn servidor_exp_time:app --host 0.0.0.0 --port 8000\",\n            ] ##ESTA LINEA SE DEBE MODIFICAR DE ACUERDO AL SERVIDOR\n          env:\n            - name: MYSQL_ROOT_PASSWORD\n              value: \"password\"\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"200m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n</code></pre>"},{"location":"6-loadbalancer/","title":"Load Balancer","text":"<p>Es un concepto utilizado en la adminstraci\u00f3n de sistemas informaticos que se refiere a la t\u00e9cnica de repartir un conjunto de peticiones en un conjunto de ordenadores, procesadores u otros recursos.</p> <p>Existen varios algoritmos que se encargan de determinar la manera en la que se reparten los mensajes. En nuestro caso, decidimos utilizar Round-Robin.  La l\u00f3gica Round Robin es un algoritmo de planificaci\u00f3n que se utiliza en sistemas operativos y aplicaciones para gestionar la ejecuci\u00f3n de procesos o tareas. Su enfoque principal es distribuir el tiempo de CPU de manera  justa entre todas las tareas pendientes. Para entender mejor c\u00f3mo opera la l\u00f3gica Round Robin, consideremos  un escenario sencillo donde hay tres tareas, numeradas como A, B y C, y cada tarea necesita un segmento de  tiempo para completarse. El algoritmo le asigna a cada tarea un intervalo de tiempo denominado quantum.  Por ejemplo, si el quantum es de 10 milisegundos, la tarea A se ejecutar\u00e1 durante los primeros 10 ms, luego pasar\u00e1  a la tarea B y as\u00ed sucesivamente. Despu\u00e9s de que la \u00faltima tarea en la cola recibe su tiempo, el ciclo  vuelve a comenzar con la primera tarea. Este enfoque garantiza que todas las tareas obtienen una parte equitativa de los recursos del sistema y evita  que una tarea monopolice la CPU indefinidamente. Si alguna tarea no se completa durante su quantum, se coloca  nuevamente en la cola para obtener otro segmento de tiempo en futuras iteraciones.</p> <p>Para ver la implementaci\u00f3n en README</p>"},{"location":"6-loadbalancer/#ha-proxy","title":"Ha-Proxy","text":""},{"location":"6-loadbalancer/#introduccion","title":"Introducci\u00f3n","text":"<p>A la hora de dise\u00f1ar un LoadBalancer, el software que utilizamos es Ha-Proxy. Es un software de c\u00f3digo  abierto que proporciona un equilibrador de carga de alta disponibilidad, que distribuye solicitudes entre muchos servidores. </p>"},{"location":"6-loadbalancer/#instalacion","title":"Instalaci\u00f3n","text":"<p>El primer paso es generar un contenedor donde en \u00e9l, crearemos el balanceador de carga Ha-Proxy. Los comandos del mismo ser\u00e1n: <pre><code>FROM haproxy:2.3  \nRUN mkdir --parents /var/lib/haproxy &amp;&amp; chown -R haproxy:haproxy /var/lib/haproxy  \nRUN mkdir /run/haproxy  \nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg  \n</code></pre> Para este archivo, se utiliza la imagen de HA-Proxy 2.3, basada en Linux Alpine. Se deben crear los dos directorios dentro del docker para que HA-Proxy pueda copiar y utilizar su configuracion.  Esto se hace con los dos renglones siguientes. Ademas se agregan permisos de lectura mediante el comando chown -R.  Por ultimo, se copia el archivo que contiene la configuracion de HA-Proxy haproxy.cfg dentro de los directorios creados anteriormente en el contenedor.</p> <p>Para poder definir el comportamiento del balanceador de carga, creamos un archivo de configuraci\u00f3n llamado haproxy.cfg,  a continuaci\u00f3n explicaremos cada una de sus secciones: <pre><code>global\n    log /dev/log    local0\n    log /dev/log    local1 notice\n    chroot /var/lib/haproxy\n    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n    stats timeout 30s\n    user haproxy\n    group haproxy\n    daemon\n</code></pre> global : En esta seccion se detallan las configuraciones globales de HA-Proxy. Aqui se detalla informacion como  por ejemplo el directorio raiz que utiliza HA-Proxy dentro del contenedor. Ademas se crea un socket para que  HA-Proxy pueda mostrar las estadisticas y se establece que la aplicacion correra en segundo plano (daemon).</p> <p><pre><code>    # Default SSL material locations\n    ca-base /etc/ssl/certs\n    crt-base /etc/ssl/private\n\n    # See: https://ssl-config.mozilla.org/#server=haproxy&amp;server-version=2.0.3&amp;config=intermediate\n        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384\n        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\n        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n</code></pre> SSL : Se establece la ubicacion predeterminada para los archivos SSL</p> <p><pre><code>defaults\n    log global\n    mode    http\n    option  httplog\n    option  dontlognull\n        timeout connect 5000\n        timeout client  50000\n        timeout server  50000\n</code></pre> defaults : Se configuran las opciones predeterminadas tanto para Backend como para Frontend (Entradas y Salidas del balanceador). Aqui se establece que se utiliza el protocolo HTTP y se descartan las conexiones que no transmiten datos. Ademas se establecen  los timeout para conexiones, cliente y servidor.</p> <p><pre><code>frontend http_web\n        bind *:2023\n        mode http\n        use_backend milu\n\nfrontend stats\n    bind *:8404\n    mode http\n    stats enable\n    stats uri /haproxy-stats  \n    stats refresh 10s\n</code></pre> frontend : Se crean dos frontends: http_web : Se establece el funcionamiento de la entrada al balanceador de carga. Se asocia el puerto :2023 para recibir peticiones y dichas peticiones se repartiran en el backend llamado milu. stats : Se establece un frontend para visualizar las estadisticas del balanceador.Se asocia el puerto :8404 para visualizar,  ademas se establecen los tiempos de actualizacion y se habilitan las estadisticas.</p> <p><pre><code>backend milu\n    mode http\n    balance roundrobin\n    option forwardfor\n    server apache1 172.17.0.4:80 check\n    server apache2 172.17.0.5:80 check\n</code></pre> backend : Se crea el backend que contiene los servidores a los cuales enviaremos las peticiones recibidas del frontend utilizando  el protocolo HTTP, y el algoritmo de balanceo round robin y se agrega la direccion del cliente en la cabecera HTTP.  Por ultimo, se configuran cada uno de los servidores a utilizar. En este caso se utilizaron servidores apache a modo de ejemplo,  con sus respectivas direcciones IP y sus puertos asociados. Ademas mediante el check, HA-Proxy verifica la disponibilidad de dichos servidores.</p>"},{"location":"6-loadbalancer/#ejecucion","title":"Ejecuci\u00f3n","text":"<p>Para crear la imagen del contenedor, utilizar la siguiente linea, situados en el mismo directorio en el que se encuentran los dos archivos creados anteriormente: <pre><code>$ docker build -t my-haproxy .\n</code></pre> Una vez creado el contenedor, es posible verificar si la configuracion asignada al archivo haproxy.cfg es correcta y no presenta errores  en la sintaxis. Para ello se puede utilizar el comando: <pre><code>$ docker run -it --rm --name haproxy-syntax-check my-haproxy haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg\n</code></pre> Para ejecutar el contenedor de HA-Proxy con la configuracion correspondiente, ejecutar el siguiente comando: <pre><code>$ docker run -d --name my-running-haproxy --sysctl net.ipv4.ip_unprivileged_port_start=0 my-haproxy\n</code></pre> Para probar utilizando un apache, se puede utilizar la siguiente linea: <pre><code>$ docker run -d -p 8081:80 --name apache1 httpd:2.4\n</code></pre> Esta linea levanta un servidor de prueba apache1 con puerto :8081. Esta configuracion se debe agregar dentro del backend del archivo haproxy.cfg. Si se varios contenedores apache y se agregan dentro del archivo de configuracion, para probar el balanceador se puede utilizar el comando curl:</p> <p><pre><code>$ curl [IP del contenedor de HA-Proxy]:2023\n</code></pre> Respuestas obtenidas: (Se modifico el index.html de un apache para distinguir las respuestas) <pre><code>\u279c  curl 172.17.0.5:2023\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n\u279c  curl 172.17.0.5:2023\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!-------2&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> Para analizar las estadisticas, se puede utilizar la siguiente direccion: <pre><code>http://[IP del contenedor de HA-Proxy]:8404/haproxy-stats\n</code></pre></p>"},{"location":"7-minicube/","title":"Kubernetes","text":"<p>Kubernetes es una plataforma de c\u00f3digo abierto para gestionar aplicaciones en contenedores, automatizando su implementaci\u00f3n, escalado y operaci\u00f3n.</p>"},{"location":"7-minicube/#minikube","title":"Minikube","text":""},{"location":"7-minicube/#introduccion","title":"Introduccion","text":"<p>Minikube es una herramienta que facilita la ejecuci\u00f3n de cl\u00fasteres de Kubernetes en entornos locales, permitiendo a los desarrolladores probar y desarrollar aplicaciones en contenedores en su propia m\u00e1quina.</p>"},{"location":"7-minicube/#requisitos","title":"Requisitos","text":"<p>2 CPU o m\u00e1s 2 GB de memoria libre 20 GB de espacio libre de disco Conexi\u00f3n a Internet Contenedor o gestor de m\u00e1quinas virtuales, tales como: Docker, QEMU, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox o VMware Fusion/Workstation</p>"},{"location":"7-minicube/#instalacion","title":"Instalacion","text":"<p><pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n</code></pre> <pre><code>sudo install minikube-linux-amd64 /usr/local/bin/minikube\n</code></pre></p>"},{"location":"7-minicube/#implementacion","title":"Implementacion","text":"<p>Desde una terminal de Linux, iniciamos minikube con el driver de virtualbox. Por defecto minikube crea un solo nodo y con el driver Docker.</p> <p><pre><code>minikube start --driver=virtualbox --memory=(ram) --cpus=(cantidad) --nodes=(cantidad de nodos)\n</code></pre> <code>--driver</code> gestor de maquinas virtuales a utilizar  <code>--memory</code> memoria ram en MB <code>--nodes</code> cantidad de nodos (maquinas virtuales) <code>--cpus</code> cantidad de nucleos de cpu  </p> <p>Ver el estado de minikube: <pre><code>minikube status\n</code></pre></p> <p>Para ver informacion sobre nuestro cluster escribimos el comando: <pre><code>minikube profile list\n</code></pre></p> <p>Para detener minikube: <pre><code>minikube stop\n</code></pre></p> <p>Para eliminar todos nuestros nodos: <pre><code>minikube delete --all\n</code></pre></p> <p>ADDONS: Conjuntos de componentes adicionales y configuraciones predefinidas que puedes habilitar f\u00e1cilmente para extender la funcionalidad de tu cl\u00faster de Kubernetes local.</p> <p>Administrar addons: <pre><code>minikube addons list\n</code></pre></p> <p>Activar addons: <pre><code>minikube addons enable (nombre del addons)\n</code></pre></p> <p>Ejemplo de algunos addons y para que sirven:</p> <ul> <li> <p>Dashboard: Proporciona una interfaz web que permite visualizar y administrar recursos en tu cl\u00faster de Kubernetes.</p> </li> <li> <p>Metrics Server: Recopila m\u00e9tricas de recursos del cl\u00faster y las hace accesibles para consultas.</p> </li> <li> <p>Registry: Configura un registro de contenedores local en el cl\u00faster, lo que puede ser \u00fatil para probar im\u00e1genes personalizadas.</p> </li> <li> <p>Ingress: Facilita la exposici\u00f3n de servicios HTTP y HTTPS desde el cl\u00faster a trav\u00e9s de reglas de enrutamiento.</p> </li> </ul>"},{"location":"7-minicube/#kubectl","title":"Kubectl","text":""},{"location":"7-minicube/#introduccion_1","title":"Introduccion","text":"<p>Kubectl se utiliza para desplegar y gestionar aplicaciones en Kubernetes. Usando kubectl, puedes inspeccionar recursos del cl\u00faster; crear, eliminar, y actualizar componentes; explorar tu nuevo cl\u00faster y arrancar aplicaciones.</p>"},{"location":"7-minicube/#instalacion_1","title":"Instalacion","text":"<p><pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n</code></pre> Validar binario: <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\n</code></pre> <pre><code>echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\n</code></pre> A la salida de este comando debe decir \"la suma coincide\".</p> <pre><code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> <p>Validar version: <pre><code>kubectl version --client\n</code></pre></p>"},{"location":"7-minicube/#implementacion_1","title":"Implementacion","text":"<p>Aplicar un archivo <code>.yaml</code> <pre><code>kubectl apply -f (archivo.yaml)\n</code></pre> Para aplicar todos los archivos <code>.yaml</code> del directorio donde nos encontramos ubicados: <pre><code>kubectl apply -f .\n</code></pre></p> <p>Visualizar servicios: <pre><code>kubectl get svc\n</code></pre></p> <p>Visualizar pods: <pre><code>kubectl get pods\n</code></pre></p> <p>Visualizar todo lo que esta corriendo nuestro cluster: <pre><code>kubectl get all\n</code></pre></p>"},{"location":"7-minicube/#archivos-yaml","title":"Archivos .yaml","text":""},{"location":"7-minicube/#introduccion_2","title":"Introduccion","text":"<p>.yaml (YAML) es un formato de serializaci\u00f3n de datos que se utiliza com\u00fanmente para representar configuraciones y datos estructurados de manera legible para humanos. La sigla \"YAML\" significa \"YAML Ain't Markup Language\" o, de manera recursiva, \"YAML Ain't a Markup Language\". YAML es un formato simple y f\u00e1cil de leer que utiliza espacios y sangr\u00edas para estructurar la informaci\u00f3n. En el contexto de Kubernetes y otros sistemas de orquestaci\u00f3n de contenedores, los archivos YAML son com\u00fanmente utilizados para definir la configuraci\u00f3n de los recursos, como despliegues, servicios, vol\u00famenes, y m\u00e1s.</p>"},{"location":"7-minicube/#tipos-de-archivos-yaml","title":"Tipos de archivos .yaml","text":"<p>Los dos que deben implementarse si o si son:</p> <ul> <li>Deployments: Un \"Deployment\" en Kubernetes es un objeto que permite declarar y gestionar la implementaci\u00f3n de aplicaciones en un cl\u00faster. Proporciona funcionalidades como estrategias de despliegue, escalabilidad y rollbacks, facilitando la gesti\u00f3n del ciclo de vida de las aplicaciones en entornos contenerizados.</li> <li>Service: Un \"Service\" en Minikube es como un operador de tr\u00e1fico para tus aplicaciones en Kubernetes. Le da a tus aplicaciones una direcci\u00f3n fija y f\u00e1cil de recordar para que puedan comunicarse entre s\u00ed sin importar d\u00f3nde est\u00e9n ejecut\u00e1ndose. Tambi\u00e9n puede repartir el tr\u00e1fico entre varias partes de tu aplicaci\u00f3n para mantener las cosas equilibradas. En resumen, hace que sea m\u00e1s f\u00e1cil para las partes de tu aplicaci\u00f3n encontrarse y hablar entre s\u00ed.</li> </ul>"},{"location":"Grupo1/1-marco_teorico/","title":"Marco te\u00f3rico","text":"<p>En este trabajo se considera un modelo CDC que admite muchas m\u00e1quinas virtuales en una m\u00e1quina f\u00edsica (PM) y varios contenedores en una m\u00e1quina virtual que refleja los escenarios de casos de uso reales en las plataformas de servicios en la nube actuales. Se asignan varios contenedores a una m\u00e1quina virtual, mientras que a un PM se le pueden asignar varias m\u00e1quinas virtuales a trav\u00e9s de un hipervisor.</p> <p>Un hypervisor, o monitor de m\u00e1quina virtual (VMM), se usa popularmente para administrar todas las m\u00e1quinas virtuales en un PM. La virtualizaci\u00f3n del servidor puede ejecutar toda la m\u00e1quina virtual (es decir, ejecutar la m\u00e1quina virtual, incluido su sistema operativo invitado, en otro sistema operativo host). Esta tecnolog\u00eda de virtualizaci\u00f3n puede garantizar un alto rendimiento, confiabilidad, confidencialidad y aislamiento de seguridad entre las instancias de VM. El aislamiento entre las m\u00e1quinas virtuales es firme y una m\u00e1quina virtual individual no tiene conocimiento de otras m\u00e1quinas virtuales que se ejecutan en el mismo PM.</p> <p></p> <p>Un contenedor mantiene el aislamiento de un cl\u00faster de procesosS de los dem\u00e1s en un sistema operativo.  Al usar, por ejemplo, namespaces de Linux, como se muestra en la figura 1, los contenedores proporcionan su vista privada adecuada del sistema operativo, las interfaces de red y el marco del sistema de archivos, que se usa en este documento. Las instancias de contenedor comparten el kernel. El sistema operativo con otros procesos del sistema y puede estar restringido hasta cierto punto para usar algunos recursos inform\u00e1ticos (CPU, RAM, etc .). De este modo, los contenedores en la nube proporcionan una capa de encapsulaci\u00f3n relativamente delgada sobre cualquier aplicaci\u00f3n alojada en ellos. En consecuencia, la implementaci\u00f3n y la creaci\u00f3n de instancias de contenedores son relativamente r\u00e1pidas.</p> <p>A pesar de los beneficios que ofrece la virtualizaci\u00f3n, para las aplicaciones que requieren una mayor flexibilidad en tiempo de ejecuci\u00f3n y menos aislamiento, es posible que la virtualizaci\u00f3n basada en VM no satisfaga todo el conjunto de requisitos de QoS. A medida que la carga de trabajo se vuelve m\u00e1s din\u00e1mica y cambia con el tiempo, ha habido un creciente inter\u00e9s en la virtualizaci\u00f3n basada en contenedores, que facilita en gran medida el movimiento de aplicaciones de software entre arquitecturas en la nube en comparaci\u00f3n con la virtualizaci\u00f3n basada en m\u00e1quinas virtuales. De hecho, un operador se ejecuta en un kernel con un aislamiento de rendimiento similar al de las m\u00e1quinas virtuales, pero sin la costosa sobrecarga de administraci\u00f3n del tiempo de ejecuci\u00f3n de la m\u00e1quina virtual.</p> <p>La contenedorizaci\u00f3n ofrece a las organizaciones y a los desarrolladores un entorno de programaci\u00f3n eficiente con una inversi\u00f3n y un coste operativo minimizados. Los enfoques de contenedorizaci\u00f3n se desarrollan en base a la t\u00e9cnica de virtualizaci\u00f3n en la que los recursos de hardware se abstraen para facilitar el intercambio de recursos inform\u00e1ticos virtuales.</p> <p>Sin embargo, los contenedores aportan muchos beneficios dif\u00edciles, sino imposibles, a tener con las m\u00e1quinas virtuales.</p> <p>En primer lugar, los contenedores comparten los recursos con el sistema host de forma mucho m\u00e1s eficiente que las m\u00e1quinas virtuales. En segundo lugar, los contenedores pueden iniciarse y detenerse en pocos segundos. En tercer lugar, la portabilidad del contenedor elimina la guerra de dependencia entre sistemas y garantiza que funcionar\u00e1 independientemente del sistema anfitri\u00f3n. En cuarto lugar, los contenedores son incre\u00edblemente ligeros, lo que permite a los usuarios ejecutar docenas (incluso m\u00e1s) de contenedores al mismo tiempo para simular un entorno de producci\u00f3n verdaderamente distribuido, lo que a menudo no es el caso de las m\u00e1quinas virtuales. En quinto lugar, los clientes finales de las aplicaciones pueden descargar y luego ejecutar sus aplicaciones complejas r\u00e1pidamente sin tener que pasar horas del dolor de instalaci\u00f3n y configuraci\u00f3n. Adem\u00e1s, el objetivo principal de un contenedor es hacer que una aplicaci\u00f3n sea completamente port\u00e1til y aut\u00f3noma, a diferencia de las m\u00e1quinas virtuales que apuntan a la virtualizaci\u00f3n total de un entorno externo.</p>"},{"location":"Grupo1/1-marco_teorico/#modelo-propuesto-en-paper","title":"Modelo propuesto en paper","text":"<p>A continuaci\u00f3n, se har\u00e1 una descripci\u00f3n de los diferentes modelos que se realizar\u00e1n en la implementaci\u00f3n del cl\u00faster. Cabe aclarar que esta se basa en el esquema te\u00f3rico presentado en el paper \u201cDynamic Scalability Model for Containerized Cloud Services\u201d.</p> <p>El usuario final env\u00eda una solicitud de ejecuci\u00f3n de tareas a trav\u00e9s de un balanceador de carga. Este \u00faltimo se encargar\u00e1 de redirigir el tr\u00e1fico recibido al PM de manera uniforme. Los requisitos de ejecuci\u00f3n de tareas de diversos usuarios de la nube se almacenan en el b\u00fafer, que se adjunta a la cola de equilibrio de carga para la asignaci\u00f3n de  recursos virtuales en el sistema en la nube. Las solicitudes de tareas recibidas en cola se dirigen a uno de los PM en la nube.</p> <p>Cada solicitud de ejecuci\u00f3n de tarea se asignar\u00e1 a un contenedor \u00fanico y cada solicitud de tarea se alojar\u00e1 en un contenedor diferente. En caso de solicitudes de tareas altas, se escalar\u00e1 verticalmente agregando un contenedor a la vez para ejecutar las tareas.</p> <p>Para modelar el CDC, usamos una red de colas abierta de Jackson, como se muestra en la Figura 2. Se supone que todos los MP de los CDC son id\u00e9nticos y que las solicitudes de usuario en cada cola de cualquier nodo de los CDC adoptan una pol\u00edtica FIFO (primero en entrar, primero en salir). Una vez finalizado el servicio, las solicitudes salen del PM y salen del sistema. En este documento, consideramos una solicitud de usuario como una unidad de c\u00e1lculo m\u00ednima (tarea) en el CDC y solo se puede ejecutar en un contenedor.</p> <p></p> <p>A continuaci\u00f3n, se muestran las gr\u00e1ficas del paper. Ahora, veremos las figuras obtenidas y analizaremos lo que se puede observar de ellas, comparandolas con las del paper. Cabe mencionar que el numero de figura se corresponde con la misma numeraci\u00f3n que el pr\u00e1ctico.</p>"},{"location":"Grupo1/1-marco_teorico/#figura-4-nro-de-figura-correspondiente-al-paper","title":"Figura 4 (Nro. de figura correspondiente al paper)","text":"<p>Es evidente que a medida que aumenta la tasa de llegada global de tareas, el tiempo de respuesta aumenta. Adem\u00e1s, se puede notar que para las cuatro configuraciones, el sistema en la nube no presenta un cambio radical cuando la tasa de llegada global de tareas se encuentra entre 5500 y 9500 tareas por segundo. Sin embargo, cuando superamos las 9500 tareas por segundo, el tiempo de respuesta del sistema cambia de manera exponencial para los cuatro casos, y a medida que disminuye el n\u00famero de instancias de contenedores, el tiempo de respuesta del sistema aumenta y alcanza 0.4 segundos a una tasa de llegada global de 10000 tareas por segundo en el caso de la primera configuraci\u00f3n.</p>"},{"location":"Grupo1/1-marco_teorico/#figura-5","title":"Figura 5","text":"<p>La Figura 5 muestra la tasa de abandono del sistema, en la cual las tareas pueden ser rechazadas debido a la falta de capacidad en la plataforma del CDC o la falta de espacio en las colas de LRS. El conteo de tareas abandonadas sigue aumentando con el aumento de la tasa de llegada de tareas por segundo. Por lo tanto, si hay una carga excesiva, puede haber un retraso que provoca que algunas tareas sean abandonadas.  Es evidente que a medida que aumenta el n\u00famero de instancias de contenedores, la tasa de abandono del sistema disminuye.</p>"},{"location":"Grupo1/1-marco_teorico/#figura-6","title":"Figura 6","text":"<p>La Figura 6 resume los resultados de nuestro modelo probado, informando sobre el rendimiento alcanzado en relaci\u00f3n con la tasa de env\u00edo de tareas deseada, utilizando las cuatro configuraciones. El n\u00famero de instancias de contenedores asignadas afecta el rendimiento del sistema.</p>"},{"location":"Grupo1/1-marco_teorico/#figura-7","title":"Figura 7","text":"<p>La Figura 7 muestra el efecto del n\u00famero de instancias de contenedores en el n\u00famero de tareas en el CDC cuando se var\u00eda la tasa global de llegada. Es evidente que cuando se trabaja con un mayor n\u00famero de contenedores (la cuarta configuraci\u00f3n) y la tasa de llegada global no supera las 5000 tareas por segundo, el CDC puede procesar m\u00e1s tareas, lo que permite reducir el n\u00famero de tareas en el CDC. En el caso de que la tasa de llegada global supere las 5000 tareas por segundo, el CDC con un mayor n\u00famero de contenedores presenta una gran cantidad de tareas en el CDC. Esto se explica en los resultados de la simulaci\u00f3n obtenidos en las Figuras 5 y 8. Cuando la utilizaci\u00f3n de la CPU es igual al 100% y el CDC contiene el mayor n\u00famero de contenedores, la tasa de abandono de tareas del CDC disminuye en comparaci\u00f3n con el CDC con menos contenedores (otras configuraciones). Las mismas observaciones se aplican a las otras cuatro configuraciones.</p>"},{"location":"Grupo1/1-marco_teorico/#figura-8","title":"Figura 8","text":"<p>Como podemos ver en la Figura 8, para una tasa de llegada global de m\u00e1s de 2000 tareas por segundo, puede notarse que cuando usamos la primera configuraci\u00f3n, podemos tener una violaci\u00f3n de los requisitos de SLA (Acuerdos de nivel de servicio). Por otro lado, para una tasa de llegada global de 2000 tareas por segundo y menos, y cuando tenemos la cuarta configuraci\u00f3n, se garantiza el cumplimiento de los requisitos de SLA. Por lo tanto, el n\u00famero de instancias de contenedores en el CDC tiene un impacto en la medida de la utilizaci\u00f3n de la CPU.</p> <p>Una vez visto y analizado la situaci\u00f3n propuesta por el paper, pasamos a nuestra implementaci\u00f3n.</p>"},{"location":"Grupo1/1-marco_teorico/#implementacion","title":"Implementaci\u00f3n","text":"<p>En la siguiente imagen se muestra el esquema completo del sistema a realizar donde pueden observarse los 3 subsistemas que componen el cl\u00faster:</p> <p></p> <p>Por otro lado, se realiz\u00f3 un codigo en Python para poder simular las mismas situaciones que se plantearon en el paper. Se encuentra en el siguiente link de GitHub: Script_Python</p> <p>Adem\u00e1s, se comparan diferencias entre el documento de referencia y la pr\u00e1ctica.</p>"},{"location":"Grupo1/1-marco_teorico/#generador-de-trafico","title":"Generador de tr\u00e1fico","text":"<p>Para simular el tr\u00e1fico se realiz\u00f3 un script en Python que permite seleccionar la cantidad de usuarios y la tasa de arribo de cada usuario en particular, es decir, que la tasa de arribo total es el producto entre ambos par\u00e1metros. </p> <p>Dada la l\u00f3gica del generador, existe una particularidad a tener en cuenta: la tasa de arribo por usuario obtenida ser\u00e1 menor a la seleccionada. Esto se da porque adem\u00e1s de esperar el tiempo requerido por la funci\u00f3n de variable de distribuci\u00f3n exponencial tambi\u00e9n se incluye el tiempo de respuesta del servidor. Esto es que un usuario espera la respuesta del servidor para generar una nueva petici\u00f3n. Este efecto desaparece cuando el tiempo medio de interarribo por usuario es considerablemente m\u00e1s grande que el tiempo medio de respuesta del servidor.</p>"},{"location":"Grupo1/1-marco_teorico/#balanceador-de-carga","title":"Balanceador de carga","text":"<p>Para dar cabida a todas las solicitudes de tareas entrantes y a la capacidad casi infinita en la nube, modelamos y aproximamos el equilibrador de carga como un  modelo de cola M/M/1 con un b\u00fafer de solicitud de tareas de capacidad infinita y llegada de una sola solicitud de tarea.</p> <p>Las llegadas de solicitudes de tareas se producen a la tasa \"lambda\" seg\u00fan un proceso de Poisson, donde las duraciones entre llegadas entre solicitudes sucesivas de  tareas que llegan suelen ser independientes y se distribuyen exponencialmente con una tasa de 1/lambda.</p> <p>Los tiempos de servicio en el servidor del equilibrador de carga se distribuyen exponencialmente con el par\u00e1metro de velocidad \"mu\" , donde 1/mu es el tiempo medio de servicio.</p> <p>En la pr\u00e1ctica, la primera \u201cM\u201d corresponde al proceso de arribo, el cual es posible gobernar debido a que se puede regular el tiempo de interarribo en el programa de Python realizado en el cliente. Respecto de la segunda \u201cM\u201d define proceso de servicio, tambi\u00e9n con distribuci\u00f3n exponencial, en la implementaci\u00f3n a realizar no se tiene certeza sobre qu\u00e9 distribuci\u00f3n se tiene, ya que depende del servicio de Haproxy en el cual nuestra capacidad de modificaci\u00f3n es limitada. En la implementaci\u00f3n, se podr\u00eda considerar determin\u00edstico, ya que la acci\u00f3n a realizar es pr\u00e1cticamente igual para todas las tareas, Haproxy modifica el stack de protocolos (capa 2, capa 3 y capa 4) y redistribuye de manera equitativa entre todas las m\u00e1quinas f\u00edsicas mediante la aplicaci\u00f3n del algoritmo de Round Robin, si se consideran peque\u00f1as variaciones en este tiempo de servicio tambi\u00e9n podr\u00eda modelarse con una distribuci\u00f3n normal. En el caso de nuestra implementaci\u00f3n solo se tiene una PM. Por \u00faltimo se considera que la cola de haproxy es suficientemente grande como para considerarla infinita a los fines del trabajo.</p> <p></p>"},{"location":"Grupo1/1-marco_teorico/#nodos","title":"Nodos","text":"<p>En esta etapa el proceso de arribo tambi\u00e9n se caracteriza con \u201cM\u201d donde la nueva tasa es igual a la tasa de arribo total dividida la cantidad de worker nodes. En cuanto al proceso de servicio, se tiene un caso similar al de haproxy donde es dif\u00edcil determinar qu\u00e9 distribuci\u00f3n gobierna este proceso. </p> <p>Por \u00faltimo es necesario hacer la aclaraci\u00f3n de que al definir la capacidad del sistema \u201cC\u201d , Kubernetes no gestiona colas de tareas, por lo que a priori C=1. Si bien esto llevar\u00eda a un sistema a pura p\u00e9rdida en cada nodo, en el comportamiento que se observa no existe p\u00e9rdida. Esto es por la acci\u00f3n del protocolo TCP sobre el que se monta HTTP que al estar orientado a la conexi\u00f3n garantiza la transmisi\u00f3n de las tareas a costa de una latencia mayor. Esto lo logra con diferentes herramientas como el control de ventana y la retransmisi\u00f3n, pudiendo generar colas tanto en el cliente como en el host del servidor.</p> <p></p>"},{"location":"Grupo1/1-marco_teorico/#podscontenedores","title":"Pods/Contenedores","text":"<p>En relaci\u00f3n al modelo adoptado para los pods y contenedores, en nuestro esquema pr\u00e1ctico se representa de igual manera que en el documento de referencia. La primera letra de la notaci\u00f3n de Kendall, la \u201cM\u201d, es la tasa de arribo dada por el lambda del modelo anterior dividido el numero de contenedores, por lo que es posible gobernarla. Por otro lado, la letra siguiente es una \u201cM\u201d tambi\u00e9n, ya que depende de la tasa de servicio  en el programa realizado para el servidor con FastApi, por lo que tambi\u00e9n es posible modificarla. Con relacion a la cantidad de servidores, en este caso se tienen \u201ck\u201d de los mismos ya que depende del n\u00famero de contenedores. La cuarta letra demuestra la capacidad del sistema y de igual manera que antes es \u201ck\u201d, generando un sistema a pura p\u00e9rdida. Las tareas no se encolan, se pierden. Un contenedor puede atender solo una tarea.</p> <p></p> <p>Con todos estos detalles en cuenta el sistema completo tambi\u00e9n puede modelarse como M/M/1 pero en este caso la tasa de servicio de todo el sistema ser\u00e1 la de un pod en particular definida como 100 r/s por la cantidad de pods.</p>"},{"location":"Grupo1/2-pruebas/","title":"Pruebas","text":"<p>En esta secci\u00f3n se presentan las pruebas realizadas durante el desarrollo de la implementaci\u00f3n pr\u00e1ctica para obtener conclusiones.</p>"},{"location":"Grupo1/2-pruebas/#prueba-1","title":"Prueba 1","text":"<p>En la primera prueba se busca identificar cuantas peticiones por segundo puede manejar el servidor al variar el n\u00famero de pods.</p> <p>En la siguiente imagen se muestra el numero de paquetes por segundo en funci\u00f3n del tiempo cuando var\u00eda la cantidad de pods que equivale a variar contenedores.</p> <p></p> <p>Puede verse claramente que hay una relaci\u00f3n lineal en el l\u00edmite de request per second (rps) con el n\u00famero de pods, siendo que se aumentaron en el siguiente orden: 2, 4, 6, 8.</p> <p>Este comportamiento no deber\u00eda sorprender si tenemos en cuenta que cada pod individualmente, gestiona de forma ideal 100 rps en promedio. Estos n\u00fameros no se logran ya que hay otros tiempos en juego en la implementaci\u00f3n que disminuyen la performance del sistema. </p> <p>En base a los datos obtenidos se realiza una estimaci\u00f3n de las rps promedio reales de un pod individualmente que oscilan entre 65 y 85 rps dependiendo de factores como las m\u00e1quinas f\u00edsicas donde se implementan, las conexiones utilizadas y otros.</p> <p>Cabe aclarar que en todos los casos se asegur\u00f3 que el servidor estuviera a m\u00e1xima capacidad seleccionando correctamente el n\u00famero de usuarios y sus respectivos \"lambda\" en el generador de tr\u00e1fico.</p>"},{"location":"Grupo1/2-pruebas/#prueba-2","title":"Prueba 2","text":"<p>En la segunda prueba el objetivo es realizar las comparaci\u00f3n del tiempo de respuesta con el modelo te\u00f3rico al variar el \"ro\" o <code>p</code>. </p> <p>Para hacer los c\u00e1lculos fue necesario fijar la tasa de servicio, que se estim\u00f3 con los datos de la prueba 1 en 75 rps por pod y, luego, se fij\u00f3 la cantidad de pod en 2, por lo tanto el sistema podr\u00e1 atender 150 rps aproximadamente.</p>"},{"location":"Grupo1/2-pruebas/#caso-1","title":"Caso 1","text":"<p>Si queremos obtener un <code>p</code> de 0.4 entonces la tasa de arribo total que debemos enviar es de = <code>p . mu</code>. Por lo tanto lambda es 60 1/s.</p> <p>El tiempo medio de respuesta te\u00f3rico en un sistema M/M/1 se puede calcular mediante la relaci\u00f3n de 1/(mu - lambda).</p> <p>Si <code>p = 0.4</code> deber\u00edamos de obtener 11.11ms. Lo que se obtuvo en la pr\u00e1ctica es:</p> <p></p>"},{"location":"Grupo1/2-pruebas/#caso-2","title":"Caso 2","text":"<p>Para <code>p</code> de 0.6 entonces la tasa de arribo total que debemos enviar es 90 1/s.</p> <p>En este caso deber\u00edamos obtener 16.66 ms de retardo medio. Se lleg\u00f3 a lo siguiente:</p> <p></p>"},{"location":"Grupo1/2-pruebas/#caso-3","title":"Caso 3","text":"<p>Para <code>p</code> de 0.8 entonces la tasa de arribo total que debemos enviar es 120 1/s.</p> <p>En este caso deber\u00edamos obtener 33.33 ms de retardo medio. Se lleg\u00f3 a lo siguiente:</p> <p></p>"},{"location":"Grupo1/2-pruebas/#caso-4","title":"Caso 4","text":"<p>Para <code>p</code> de 0.9 entonces la tasa de arribo total que debemos enviar es 135 1/s.</p> <p>En este caso deber\u00edamos obtener 66.66 ms de retardo medio. Se lleg\u00f3 a lo siguiente:</p> <p></p>"},{"location":"Grupo1/2-pruebas/#caso-5","title":"Caso 5","text":"<p>Para <code>p</code> de 0.99 la tasa de arribo total que debemos enviar es 148 1/s.</p> <p></p>"},{"location":"Grupo1/2-pruebas/#graficas","title":"Graficas","text":"<p>La siguiente tabla nos muestra los valores de Tiempo medio de servicio reales e ideales correspondientes a distintos valores de \u03c1.</p> <p></p> <p>Con los valores anteriores se realizo la siguiente grafica para poder visualizar las diferencias entre la implementacion y los calculos teoricos.</p> <p></p>"},{"location":"Grupo1/2-pruebas/#caso-de-prueba-teorico","title":"Caso de prueba teorico","text":"<p>Si <code>p &gt; 1</code>, por ejemplo, que genere una tasa de arribo de 1000 tareas por segundo, las formulas teoricas no aplican y el modelo tendria un delay infinito.</p> <p></p> <p>En el caso de la implementacion real este fenomeno se observa en el hecho de que cuando se utiliza p &gt; 1 el calculo de tiempo medio de respuesta del sistema no converge para ninguna cantidad x de paquetes incluidos en el calculo de la media, se presenta una tendencia de crecimiento hacia el \"infinito\", y la velocidad de esta esta determinada por que tanto supera p a 1, pero a fines practicos sabemos que existe algun limite ya sea por el hardware o por el timeout de TCP y partir de ese punto existira perdida de paquetes. Este analisis no esta considerado dentro de nuestro modelado.</p>"},{"location":"Grupo1/3-conclusiones/","title":"Conclusiones","text":"<p>El an\u00e1lisis de las conclusiones puede hacerse desde dos enfoques diferentes, uno para la implementaci\u00f3n y el otro para el estudio de los datos obtenidos.</p> <p>En cuanto a la implementaci\u00f3n, se consigui\u00f3 a lo largo de este trabajo adquirir los conceptos necesarios para una correcta utilizaci\u00f3n de las tecnolog\u00edas para montar un cluster b\u00e1sico localmente, fue necesario principalmente aprender sobre funciones s\u00edncronas y as\u00edncronas en python as\u00ed como mejorar nuestro entendimiento sobre dicho lenguaje, tambi\u00e9n se logr\u00f3 comprender gran parte del funcionamiento de kubernetes y la utilizaci\u00f3n de esta plataforma open source localmente con la ayuda de minikube, tambi\u00e9n fue necesario aprender el funcionamiento b\u00e1sico de FastAPI para montar un servicio, Locust para generar tr\u00e1fico y graficar informaci\u00f3n \u00fatil y Haproxy para lograr un balanceador de carga.</p> <p>Por otro lado se encuentra el intento de aproximar el sistema generado con el modelo te\u00f3rico del paper, aqu\u00ed se encontraron ciertas dificultades dadas por las caracter\u00edsticas de las tecnolog\u00edas mencionadas anteriormente, por las cuales el modelado final no result\u00f3 exactamente igual al deseado, de toda formas se pudo sortear esta problem\u00e1tica y repensar un modelo m\u00e1s adecuado para luego realizar un an\u00e1lisis te\u00f3rico y de pruebas sobre el mismo, obteniendo resultados suficientemente similares en las pruebas y en los c\u00e1lculos tanto de tiempo de respuesta como de capacidad del sistema.</p> <p>Como trabajo futuro, se podr\u00eda considerar la implementaci\u00f3n f\u00edsica de este modelo en un cl\u00faster real para evaluar las posibles diferencias de rendimiento en comparaci\u00f3n con las implementaciones realizadas en este trabajo de investigaci\u00f3n. Adem\u00e1s, ser\u00eda beneficioso explorar la implementaci\u00f3n din\u00e1mica de la generaci\u00f3n de pods, con el objetivo de atender autom\u00e1ticamente las demandas del cliente y optimizar eficientemente los recursos disponibles.</p>"},{"location":"Grupo1/4-primer_parcial/","title":"C\u00f3digos realizados para la obtenci\u00f3n de graficas","text":""},{"location":"Grupo1/4-primer_parcial/#figura-4","title":"Figura 4","text":"<pre><code># Librerias a utilizar\nimport math                           #Para calcular el factorial de un nro\nimport matplotlib.pyplot as plt       #Para graficar los resultados obtenidos\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# FIGURE 4\n# Par\u00e1metros del sistema\npm = list(range(2, 5 + 1, 1))                      # m\u00e1quinas f\u00edsicas\nvm = 10                                            # m\u00e1quinas virtuales (n)\ncnt = list(range(1, 18 + 1, 1))                    # contenedores (k)\nlambd = list(range(1000, 9982 + 1, 499))           # tasa de arribos al CDC\nc = 300                                            # El n\u00famero m\u00e1ximo de solicitudes de ejecuci\u00f3n de tareas en la cola de LRS.\nsobu = 0.0001                                      # 1/\u03bc La duraci\u00f3n del servicio de cada solicitud de ejecuci\u00f3n de tareas en la cola del balanceador de carga.\nsobu1 = 0.001                                      # 1/\u03bc1 La duraci\u00f3n promedio de servicio de cada tarea en la cola de LRS es de 0.001 segundos.\nsobu2 = 0.01                                       # 1/\u03bc2 Las duraciones de ejecuci\u00f3n de tareas en los contenedores de cada MV son variables aleatorias exponenciales independientes e id\u00e9nticamente distribuidas con una duraci\u00f3n promedio de 0.01 segundos.\nu = 1/sobu\nu1 = 1/sobu1\nu2 = 1/sobu2 \ncolores = ['b', 'g', 'r', 'c', 'm']                # Colores para graficar\nresponse_data1 = []                                # Vectores para guardar los datos y graficar\nresponse_data2 = []\nresponse_data3 = []\nresponse_data4 = []\n\n# Variacion de las m\u00e1quinas fisicas\nfor pim in pm:\n\n    # Variacion del lambda\n    for globalarribalrate in lambd:\n        # LOAD BALANCER\n        elb = globalarribalrate/(u-globalarribalrate)                                       # Numero medio de tareas\n        tlb = elb/globalarribalrate                                                         # Tiempo medio de respuesta\n\n        # LRS\n        lambd1 = globalarribalrate/pim                                                      # lambda_1  tasa de arribo a las PM\n        filrs = lambd1/u1                                                                   # Carga en el buffer\n        pjlosslrs = lambd1 * ((1-filrs)/(1-pow(filrs,(c+1))))*pow(filrs,c)                  # Tareas perdidas\n        djlrs = lambd1 * ((1-(filrs)**c)/(1-(filrs)**(c+1)))                                # Rendimiento \n        xjlrs = (filrs/(1-filrs))*((1-((c+1)*(filrs)**c)+c*filrs**(c+1))/(1-filrs**(c+1)))  # Tareas en la PM\n        ljlrs = 1 - ((1-filrs)/(1-filrs**(c+1)))                                            # Tareas en servicio\n        mjlrs = xjlrs - ljlrs                                                               # Tareas esperando en el buffer\n        cpulrs = djlrs/u1                                                                   # Utilizacion del CPU\n        wjlrs = mjlrs/lambd1                                                                # Tiempo de espera por tareas\n        tjlrs = xjlrs/djlrs                                                                 # Tiempo de respuesta por tarea\n\n        # CONTAINER\n        lambd2 = lambd1/vm                                                                  # lambda_2 tasa de arribos a los contenedores\n        ficnt = lambd2/u2                                                                   # Tasa de arribos\n        sumatoria2 = 0\n        for ki in cnt:                                                                      # Acumulamos los valores para obtener la sumatoria\n            sumatoria = (ficnt**ki)/math.factorial(ki)\n            sumatoria2 = sumatoria + sumatoria2\n\n        pilosscnt = ((ficnt**ki)/math.factorial(ki))/sumatoria2                             # Probabilidad de perdida\n        xicnt = ficnt*(1-pilosscnt)                                                         # Numero de tareas en la iesima VM\n        ticnt = 1/u2                                                                        # Tiempo de respuesta en la iesima VM\n\n        sysresponse = tlb + tjlrs + ticnt\n\n        if  pim-1==1:\n            response_data1.append(sysresponse)\n        elif  pim-1==2:\n            response_data2.append(sysresponse)\n        elif  pim-1==3:\n            response_data3.append(sysresponse)\n        elif  pim-1==4:\n            response_data4.append(sysresponse)   \n\nplt.figure(figsize=(10, 6))\nplt.plot(lambd, response_data1, label='20 VM')\nplt.plot(lambd, response_data2, label='30 VM')\nplt.plot(lambd, response_data3, label='40 VM')\nplt.plot(lambd, response_data4, label='50 VM')\nplt.xlabel('Tasa de arribos al CDC [tareas/s]')\nplt.ylabel('Tiempo de respuesta del sistema [s]')\nplt.title('Efecto de la tasa de arribos en el tiempo de respuesta')\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Grupo1/4-primer_parcial/#figura-5","title":"Figura 5","text":"<pre><code># Librerias a utilizar\nimport math                           #Para calcular el factorial de un nro\nimport matplotlib.pyplot as plt       #Para graficar los resultados obtenidos\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# FIGURE 5\n# Par\u00e1metros del sistema\npm = list(range(2, 5 + 1, 1))                      # m\u00e1quinas f\u00edsicas\nvm = 10                                            # m\u00e1quinas virtuales (n)\ncnt = list(range(1, 18 + 1, 1))                    # contenedores (k)\nlambd = list(range(1000, 9982 + 1, 499))           # tasa de arribos al CDC\nc = 300                                            # El n\u00famero m\u00e1ximo de solicitudes de ejecuci\u00f3n de tareas en la cola de LRS.\nsobu = 0.0001                                      # 1/\u03bc La duraci\u00f3n del servicio de cada solicitud de ejecuci\u00f3n de tareas en la cola del balanceador de carga.\nsobu1 = 0.001                                      # 1/\u03bc1 La duraci\u00f3n promedio de servicio de cada tarea en la cola de LRS es de 0.001 segundos.\nsobu2 = 0.01                                       # 1/\u03bc2 Las duraciones de ejecuci\u00f3n de tareas en los contenedores de cada MV son variables aleatorias exponenciales independientes e id\u00e9nticamente distribuidas con una duraci\u00f3n promedio de 0.01 segundos.\nu = 1/sobu\nu1 = 1/sobu1\nu2 = 1/sobu2\ncolores = ['b', 'g', 'r', 'c', 'm']                # Colores para graficar\nresponse_data1 = []                                # Vectores para guardar los datos y graficar\nresponse_data2 = []\nresponse_data3 = []\nresponse_data4 = []\n\n# Variacion de las m\u00e1quinas fisicas\nfor pim in pm:\n\n    # Variacion del lambda\n    for globalarribalrate in lambd:\n\n        # LOAD BALANCER\n        elb = globalarribalrate/(u-globalarribalrate)                                         # Numero medio de tareas\n        tlb = elb/globalarribalrate                                                           # Tiempo medio de respuesta\n\n        # LRS\n        lambd1 = globalarribalrate/pim                                                        # lambda_1  tasa de arribo a las PM\n        filrs = lambd1/u1                                                                     # Carga en el buffer\n        pjlosslrs =  ((1-filrs)*filrs**c)/(1-filrs**(c+1))                                    # Tareas perdidas\n        djlrs = lambd1 * ((1-(filrs)**c)/(1-(filrs)**(c+1)))                                  # Rendimiento \n        xjlrs = (filrs/(1-filrs))*((1-((c+1)*(filrs)**c)+c*filrs**(c+1))/(1-(filrs**(c+1))))  # Tareas en la PM\n        ljlrs = 1 - ((1-filrs)/(1-(filrs**(c+1))))                                            # Tareas en servicio\n        mjlrs = xjlrs - ljlrs                                                                 # Tareas esperando en el buffer\n        cpulrs = djlrs/u1                                                                     # Utilizacion del CPU\n        wjlrs = mjlrs/lambd1                                                                  # Tiempo de espera por tareas\n        tjlrs = xjlrs/djlrs                                                                   # Tiempo de respuesta por tarea\n\n        # CONTAINER\n        containers = 18\n        lambd2 = lambd1/vm                                                                    # lambda_2 tasa de arribos a los contenedores\n        ficnt = lambd2/u2                                                                     # Tasa de arribos\n        numerator = (lambd2 ** containers) / math.factorial(containers)\n        denominator = sum((lambd2 ** i) / math.factorial(i) for i in range(containers + 1))\n        pilo= numerator / denominator\n        xicnt = ficnt*(1-pilosscnt)                                                           # Numero de tareas en la iesima VM\n        ticnt = 1/u2                                                                          # Tiempo de respuesta en la iesima VM\n        ptotal = pilosscnt + pjlosslrs\n        sysresponse = ptotal * globalarribalrate\n\n        if  pim-1==1:\n            response_data1.append(sysresponse)\n        elif  pim-1==2:\n            response_data2.append(sysresponse)\n        elif  pim-1==3:\n            response_data3.append(sysresponse)\n        elif  pim-1==4:\n            response_data4.append(sysresponse)   \n\nplt.figure(figsize=(10, 6))\nplt.plot(lambd, response_data1, label='20 VM')\nplt.plot(lambd, response_data2, label='30 VM')\nplt.plot(lambd, response_data3, label='40 VM')\nplt.plot(lambd, response_data4, label='50 VM')\nplt.xlabel('Tasa de arribos al CDC [tareas/segundo]')\nplt.ylabel('Tareas perdidas en el sistema [tareas]')\nplt.title('Efecto de la tasa de arribos en las perdidas')\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Grupo1/4-primer_parcial/#figura-6","title":"Figura 6","text":"<pre><code># Librerias a utilizar\nimport math                           #Para calcular el factorial de un nro\nimport matplotlib.pyplot as plt       #Para graficar los resultados obtenidos\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# FIGURE 6\n# Par\u00e1metros del sistema\npm = list(range(2, 5 + 1, 1))                      # m\u00e1quinas f\u00edsicas\nvm = 10                                            # m\u00e1quinas virtuales (n)\ncnt = list(range(1, 18 + 1, 1))                    # contenedores (k)\nlambd = list(range(1000, 9982 + 1, 499))           # tasa de arribos al CDC\nc = 300                                            # El n\u00famero m\u00e1ximo de solicitudes de ejecuci\u00f3n de tareas en la cola de LRS.\nsobu = 0.0001                                      # 1/\u03bc La duraci\u00f3n del servicio de cada solicitud de ejecuci\u00f3n de tareas en la cola del balanceador de carga.\nsobu1 = 0.001                                      # 1/\u03bc1 La duraci\u00f3n promedio de servicio de cada tarea en la cola de LRS es de 0.001 segundos.\nsobu2 = 0.01                                       # 1/\u03bc2 Las duraciones de ejecuci\u00f3n de tareas en los contenedores de cada MV son variables aleatorias exponenciales independientes e id\u00e9nticamente distribuidas con una duraci\u00f3n promedio de 0.01 segundos.\nu = 1/sobu\nu1 = 1/sobu1\nu2 = 1/sobu2\ncolores = ['b', 'g', 'r', 'c', 'm']                # Colores para graficar\nresponse_data1 = []                                # Vectores para guardar los datos y graficar\nresponse_data2 = []\nresponse_data3 = []\nresponse_data4 = []\n\n# Variacion de las m\u00e1quinas fisicas\nfor pim in pm:\n\n    # Variacion del lambda\n    for globalarribalrate in lambd:\n\n        # LOAD BALANCER\n        elb = globalarribalrate/(u-globalarribalrate)                                         # Numero medio de tareas\n        tlb = elb/globalarribalrate                                                           # Tiempo medio de respuesta\n\n        # LRS\n        lambd1 = globalarribalrate/pim                                                        # lambda_1  tasa de arribo a las PM\n        filrs = lambd1/u1                                                                     # Carga en el buffer\n        pjlosslrs = ((1-filrs)*filrs**c)/(1-filrs**(c+1))                                     # Tareas perdidas\n        djlrs =  globalarribalrate * ((1-(filrs)**c)/(1-(filrs)**(c+1)))                      # Rendimiento \n        xjlrs = (filrs/(1-filrs))*((1-((c+1)*(filrs)**c)+c*filrs**(c+1))/(1-(filrs**(c+1))))  # Tareas en la PM\n        ljlrs = 1 - ((1-filrs)/(1-(filrs**(c+1))))                                            # Tareas en servicio\n        mjlrs = xjlrs - ljlrs                                                                 # Tareas esperando en el buffer\n        cpulrs = djlrs/u1                                                                     # Utilizacion del CPU\n        wjlrs = mjlrs/lambd1                                                                  # Tiempo de espera por tareas\n        tjlrs = xjlrs/djlrs                                                                   # Tiempo de respuesta por tarea\n\n        # CONTAINER\n        lambd2 = lambd1/vm                                                                    # lambda_2 tasa de arribos a los contenedores\n        ficnt = lambd2/u2                                                                     # Tasa de arribos\n        sumatoria2 = (ficnt**18)/math.factorial(18)\n        for ki in cnt:                                                                        # Acumulamos los valores para obtener la sumatoria\n            sumatoria = (ficnt**ki)/math.factorial(ki)\n            sumatoria2 = sumatoria + sumatoria2\n\n        pilosscnt = ((ficnt**18)/(math.factorial(18)))/(sumatoria2)                           # Probabilidad de perdida\n\n        xicnt = ficnt*(1-pilosscnt)                                                           # Numero de tareas en la iesima VM\n        ticnt = 1/u2                                                                          # Tiempo de respuesta en la iesima VM\n\n        sysresponse = djlrs\n\n        if  pim-1==1:\n            response_data1.append(sysresponse)\n        elif  pim-1==2:\n            response_data2.append(sysresponse)\n        elif  pim-1==3:\n            response_data3.append(sysresponse)\n        elif  pim-1==4:\n            response_data4.append(sysresponse)   \n\nplt.figure(figsize=(10, 6))\nplt.plot(lambd, response_data1, label='20 VM')\nplt.plot(lambd, response_data2, label='30 VM')\nplt.plot(lambd, response_data3, label='40 VM')\nplt.plot(lambd, response_data4, label='50 VM')\nplt.xlabel('Tasa de arribos al CDC [tareas/s]')\nplt.ylabel('Tareas procesadas en el sistema [tareas/s]')\nplt.title('Efecto de la tasa de arribos en el procesamiento del sistema')\nplt.legend(loc='best')\nplt.grid(True) \nplt.show()\n</code></pre>"},{"location":"Grupo1/4-primer_parcial/#figura-7","title":"Figura 7","text":"<pre><code># Librerias a utilizar\nimport math                           #Para calcular el factorial de un nro\nimport matplotlib.pyplot as plt       #Para graficar los resultados obtenidos\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# FIGURE 7\n# Par\u00e1metros del sistema\npm = list(range(2, 5 + 1, 1))                      # m\u00e1quinas f\u00edsicas\nvm = 10                                            # m\u00e1quinas virtuales (n)\ncnt = list(range(1, 18 + 1, 1))                    # contenedores (k)\nlambd = list(range(1000, 9982 + 1, 499))           # tasa de arribos al CDC\nc = 300                                            # El n\u00famero m\u00e1ximo de solicitudes de ejecuci\u00f3n de tareas en la cola de LRS.\nsobu = 0.0001                                      # 1/\u03bc La duraci\u00f3n del servicio de cada solicitud de ejecuci\u00f3n de tareas en la cola del balanceador de carga.\nsobu1 = 0.001                                      # 1/\u03bc1 La duraci\u00f3n promedio de servicio de cada tarea en la cola de LRS es de 0.001 segundos.\nsobu2 = 0.01                                       # 1/\u03bc2 Las duraciones de ejecuci\u00f3n de tareas en los contenedores de cada MV son variables aleatorias exponenciales independientes e id\u00e9nticamente distribuidas con una duraci\u00f3n promedio de 0.01 segundos.\nu = 1/sobu\nu1 = 1/sobu1\nu2 = 1/sobu2\ncolores = ['b', 'g', 'r', 'c', 'm']                # Colores para graficar\nresponse_data1 = []                                # Vectores para guardar los datos y graficar\nresponse_data2 = []\nresponse_data3 = []\nresponse_data4 = []\n\n# Variacion de las m\u00e1quinas fisicas\nfor pim in pm:\n\n    # Variacion del lambda\n    for globalarribalrate in lambd:\n        # c = pim * containers * 10\n        # LOAD BALANCER\n        elb = globalarribalrate/(u-globalarribalrate)                                         # Numero medio de tareas\n        tlb = elb/globalarribalrate                                                           # Tiempo medio de respuesta\n\n        # LRS\n        lambd1 = globalarribalrate/pim                                                        # lambda_1  tasa de arribo a las PM\n        filrs = lambd1/u1                                                                     # Carga en el buffer\n        pjlosslrs = ((1-filrs)*filrs**c)/(1-filrs**(c+1))                                     # Tareas perdidas\n        djlrs =  globalarribalrate * ((1-(filrs)**c)/(1-(filrs)**(c+1)))                      # Rendimiento \n        xjlrs = (filrs/(1-filrs))*((1-((c+1)*(filrs)**c)+c*filrs**(c+1))/(1-(filrs**(c+1))))  # Tareas en la PM\n        ljlrs = 1 - ((1-filrs)/(1-(filrs**(c+1))))                                            # Tareas en servicio\n        mjlrs = xjlrs - ljlrs                                                                 # Tareas en una maquina fisica\n        cpulrs = djlrs/u1                                                                     # Utilizacion del CPU\n        wjlrs = mjlrs/lambd1                                                                  # Tiempo de espera por tareas\n        tjlrs = xjlrs/djlrs                                                                   # Tiempo de respuesta por tarea\n\n        # CONTAINER\n        lambd2 = lambd1/vm                                                                    # lambda_2 tasa de arribos a los contenedores\n        ficnt = lambd2/u2                                                                     # Tasa de arribos\n        sumatoria2 = (ficnt**18)/math.factorial(18)\n        for ki in cnt:                                                                        # Acumulamos los valores para obtener la sumatoria\n            sumatoria = (ficnt**ki)/math.factorial(ki)\n            sumatoria2 = sumatoria + sumatoria2\n\n        pilosscnt = ((ficnt**18)/(math.factorial(18)))/(sumatoria2)                           # Probabilidad de perdida\n\n        xicnt = ficnt*(1-pilosscnt)                                                           # Numero de tareas en la iesima VM\n        ticnt = 1/u2                                                                          # Tiempo de respuesta en la iesima VM\n        ptotal = pilosscnt + pjlosslrs\n\n        sysresponse =  mjlrs * pim + elb\n\n        if  pim-1==1:\n            response_data1.append(sysresponse)\n        elif  pim-1==2:\n            response_data2.append(sysresponse)\n        elif  pim-1==3:\n            response_data3.append(sysresponse)\n        elif  pim-1==4:\n            response_data4.append(sysresponse)   \n\nplt.figure(figsize=(10, 6))\nplt.plot(lambd, response_data1, label='20 VM')\nplt.plot(lambd, response_data2, label='30 VM')\nplt.plot(lambd, response_data3, label='40 VM')\nplt.plot(lambd, response_data4, label='50 VM')\nplt.xlabel('Tasa de arribos al CDC [tareas/s]')\nplt.ylabel('Numero de tareas en el sistema [tareas]')\nplt.title('Efecto de la tasa de arribos en las tareas')\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Grupo1/4-primer_parcial/#figura-8","title":"Figura 8","text":"<pre><code># Librerias a utilizar\nimport math                           #Para calcular el factorial de un nro\nimport matplotlib.pyplot as plt       #Para graficar los resultados obtenidos\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# FIGURE 8\n# Par\u00e1metros del sistema\npm = list(range(2, 5 + 1, 1))                      # m\u00e1quinas f\u00edsicas\nvm = 10                                            # m\u00e1quinas virtuales (n)\ncnt = list(range(1, 18 + 1, 1))                    # contenedores (k)\nlambd = list(range(1000, 9982 + 1, 499))           # tasa de arribos al CDC\nc = 300                                            # El n\u00famero m\u00e1ximo de solicitudes de ejecuci\u00f3n de tareas en la cola de LRS.\nsobu = 0.0001                                      # 1/\u03bc La duraci\u00f3n del servicio de cada solicitud de ejecuci\u00f3n de tareas en la cola del balanceador de carga.\nsobu1 = 0.001                                      # 1/\u03bc1 La duraci\u00f3n promedio de servicio de cada tarea en la cola de LRS es de 0.001 segundos.\nsobu2 = 0.01                                       # 1/\u03bc2 Las duraciones de ejecuci\u00f3n de tareas en los contenedores de cada MV son variables aleatorias exponenciales independientes e id\u00e9nticamente distribuidas con una duraci\u00f3n promedio de 0.01 segundos.\nu = 1/sobu\nu1 = 1/sobu1\nu2 = 1/sobu2\ncolores = ['b', 'g', 'r', 'c', 'm']                # Colores para graficar\nresponse_data1 = []                                # Vectores para guardar los datos y graficar\nresponse_data2 = []\nresponse_data3 = []\nresponse_data4 = []\n\n# Variacion de las m\u00e1quinas fisicas\nfor pim in pm:\n\n    # Variacion del lambda\n    for globalarribalrate in lambd:\n        # LOAD BALANCER\n        elb = globalarribalrate/(u-globalarribalrate)                                       # Numero medio de tareas\n        tlb = elb/globalarribalrate                                                         # Tiempo medio de respuesta\n\n        # LRS\n        lambd1 = globalarribalrate/pim                                                      # lambda_1  tasa de arribo a las PM\n        filrs = lambd1/u1                                                                   # Carga en el buffer\n        pjlosslrs = lambd1 * ((1-filrs)/(1-pow(filrs,(c+1))))*pow(filrs,c)                  # Tareas perdidas\n        djlrs = lambd1 * ((1-(filrs)**c)/(1-(filrs)**(c+1)))                                # Rendimiento \n        xjlrs = (filrs/(1-filrs))*((1-((c+1)*(filrs)**c)+c*filrs**(c+1))/(1-filrs**(c+1)))  # Tareas en la PM\n        ljlrs = 1 - ((1-filrs)/(1-filrs**(c+1)))                                            # Tareas en servicio\n        mjlrs = xjlrs - ljlrs                                                               # Tareas esperando en el buffer\n        cpulrs = djlrs/u1                                                                   # Utilizacion del CPU\n        wjlrs = mjlrs/lambd1                                                                # Tiempo de espera por tareas\n        tjlrs = xjlrs/djlrs                                                                 # Tiempo de respuesta por tarea\n\n        # CONTAINER\n        lambd2 = lambd1/vm                                                                  # lambda_2 tasa de arribos a los contenedores\n        ficnt = lambd2/u2                                                                   # Tasa de arribos\n        sumatoria2 = 1\n        for ki in cnt:                                                                      # Acumulamos los valores para obtener la sumatoria\n            sumatoria = (ficnt**ki)/math.factorial(ki)\n            sumatoria2 = sumatoria + sumatoria2\n\n        pilosscnt = ((ficnt**18)/math.factorial(18))/sumatoria2                             # Probabilidad de perdida\n        xicnt = ficnt*(1-pilosscnt)                                                         # Numero de tareas en la iesima VM\n        ticnt = 1/u2                                                                        # Tiempo de respuesta en la iesima VM\n\n        sysresponse = (djlrs/u1)*100\n\n        if  pim-1==1:\n            response_data1.append(sysresponse)\n        elif  pim-1==2:\n            response_data2.append(sysresponse)\n        elif  pim-1==3:\n            response_data3.append(sysresponse)\n        elif  pim-1==4:\n            response_data4.append(sysresponse)   \nplt.figure(figsize=(10, 6))\nplt.plot(lambd, response_data1, label='20 VM')\nplt.plot(lambd, response_data2, label='30 VM')\nplt.plot(lambd, response_data3, label='40 VM')\nplt.plot(lambd, response_data4, label='50 VM')\nplt.xlabel('Tasa de arribos al CDC [tareas/s]')\nplt.ylabel('Uso del CPU (%)')\nplt.title('Efecto de la tasa de arribos en el uso del CPU')\nplt.legend(loc='best')\nplt.grid(True) \nplt.show()\n</code></pre>"},{"location":"Grupo2/1-teoria/","title":"Teoria","text":"<p>En primeras instancias se procede a realizar una breve introducci\u00f3n del trabajo propuesto por la c\u00e1tedra, el mismo, plantea que la computaci\u00f3n en la nube es un modelo inform\u00e1tico popular debido a su eficiencia en costos, escalabilidad y seguridad. Se basa en el acceso a recursos compartidos configurables a trav\u00e9s de Internet. La tecnolog\u00eda de virtualizaci\u00f3n, que crea instancias de m\u00e1quinas virtuales, desempe\u00f1a un papel clave.</p> <p>Las m\u00e1quinas virtuales, gestionadas por un hipervisor, son esenciales para la gesti\u00f3n de cargas de trabajo en centros de datos y nubes. La virtualizaci\u00f3n del servidor garantiza rendimiento y seguridad al ejecutar m\u00e1quinas virtuales completas en un sistema operativo host. La firmeza del aislamiento asegura que cada m\u00e1quina virtual funcione sin conocer otras en la misma m\u00e1quina f\u00edsica.</p> <p>Dicho esto, se propone implementar diferentes escenarios con diferentes tipos de sistemas, como M/M1, M/M/1/C y M/M/k/k, en donde estos son modelos matem\u00e1ticos utilizados en teor\u00eda de colas para analizar el rendimiento y el comportamiento de sistemas de espera, representados por notaciones de Kendall. Cada letra en la notaci\u00f3n representa una caracter\u00edstica espec\u00edfica del sistema. Es decir por ejemplo, un sistema M/M/k/k, significa:</p> <p>1) M: Indica que la llegada de solicitudes al sistema sigue un proceso de Poisson, lo que significa que las llegadas son aleatorias e independientes en el tiempo. La tasa de llegada se denota como \u03bb (lambda). 2) M: Representa que el servicio de las solicitudes sigue un proceso de Poisson, similar a la llegada. La tasa de servicio se denota como \u03bc (mu). 3) k: Representa el n\u00famero total de servidores disponibles en el sistema. 4) k: Indica la capacidad m\u00e1xima del sistema, es decir, el n\u00famero m\u00e1ximo de solicitudes que pueden estar en el sistema simult\u00e1neamente.</p> <p>Para esto, se implementaron m\u00e1quinas f\u00edsicas y m\u00e1quinas virtuales, para poder simular los distintos modelos antes nombrado, en el Paper descrito, realiza una peque\u00f1a introducci\u00f3n de las respectivas herramientas utilizadas para la implementaci\u00f3n del mismo.</p> <p>Una m\u00e1quina virtual (VM, por sus siglas en ingl\u00e9s) es un entorno de ejecuci\u00f3n virtual que opera como una computadora independiente dentro de otra computadora f\u00edsica. En lugar de depender directamente del hardware subyacente, una m\u00e1quina virtual se ejecuta en un software de virtualizaci\u00f3n que simula el hardware y proporciona un entorno completo del sistema operativo. Esto nos da diferentes \u201cventajas\u201d como por ejemplo, independencia de hardware, aislamiento, portabilidad, e incluso poder realizar desarrollos y pruebas.</p> <p></p> <p>Cabe destacar que existen diferentes propuestas de MV, entre estas se destacan VirtualBox,VMware Player (Windows, Linux), VMware Fusion (Mac OS X) y Parallels Desktop, e incluso Kernel-based Virtual Machine o KVM, es una soluci\u00f3n para implementar virtualizaci\u00f3n completa con Linux. Dentro de una m\u00e1quina virtual (VM), puedes realizar una amplia variedad de tareas y actividades, ya que la VM simula un entorno de computadora independiente dentro de tu sistema host, por ende es posible instalar diferentes distribuciones de sistemas operativos, diferentes aplicaciones y programas e incluso se pueden crear contenedores utilizando tecnolog\u00edas de contenedorizaci\u00f3n como Docker. Los contenedores son entornos ligeros y port\u00e1tiles que contienen aplicaciones y sus dependencias, permitiendo que se ejecuten de manera consistente en diferentes entornos, permite ejecutar aplicaciones y sus dependencias de manera aislada, pero sin la necesidad de virtualizar un sistema operativo completo. Todo comienza con las im\u00e1genes del contenedor. Una imagen es un paquete ejecutable ligero que incluye c\u00f3digo, bibliotecas, configuraciones y cualquier otra cosa necesaria para que la aplicaci\u00f3n se ejecute. Las im\u00e1genes se crean a partir de un archivo llamado Dockerfile.</p> <p>Tambi\u00e9n existe lo que se denomina el motor de contenedores, como Docker, es responsable de construir, ejecutar y gestionar contenedores. Proporciona una interfaz de l\u00ednea de comandos y una API que permite a los usuarios interactuar con contenedores y gestionar im\u00e1genes. Cuando se inicia un contenedor, se crea una instancia para la ejecuci\u00f3n de una imagen. Esto se llama \"crear un contenedor a partir de una imagen\", las im\u00e1genes de contenedor suelen almacenarse en un registro, como Docker Hub. Esto facilita la distribuci\u00f3n y el uso compartido de im\u00e1genes entre diferentes sistemas y equipos.  Las im\u00e1genes pueden tener m\u00faltiples versiones y etiquetas, lo que permite a los desarrolladores y operadores gestionar las versiones de las aplicaciones y realizar actualizaciones controladas.</p> <p>Por otro lado, cabe destacar como se ha mencionado en la secci\u00f3n de \u201cIntroducci\u00f3n\u201d en el sitio presente, se plantea una forma de tratar de elaborar y de replicar  el Paper, antes mencionado, dicho esto, para la implementaci\u00f3n del mismo se utiliz\u00f3 el concepto de Cluster,  el mismo, es un conjunto o grupo de computadoras interconectadas que trabajan juntas para realizar tareas o funciones espec\u00edficas como si fueran una sola entidad. Los cl\u00fasteres son utilizados para mejorar el rendimiento, la disponibilidad y la escalabilidad de sistemas inform\u00e1ticos. Para replicarlo, se utiliz\u00f3 m\u00e1s precisamente kubernetes, com\u00fanmente conocido como \"K8s\", es una plataforma de c\u00f3digo abierto dise\u00f1ada para automatizar la implementaci\u00f3n, el escalado y la gesti\u00f3n de aplicaciones en contenedores, en donde a partir del mismo se pudo orquestar contenedores, realizar escalado y despliegue, ya que permite escalar autom\u00e1ticamente la cantidad de r\u00e9plicas de una aplicaci\u00f3n en funci\u00f3n de la carga de trabajo. Tambi\u00e9n facilita el despliegue de nuevas versiones de aplicaciones sin tiempo de inactividad, gestionar recursos, automatizar tareas, entre otras cosas. </p> <p>En el Papers, se plantea un modelo de cola para aplicaciones en la nube que asegura el rendimiento incluso con cargas de trabajo din\u00e1micas. Su enfoque implica la migraci\u00f3n de contenedores en tiempo de ejecuci\u00f3n para cumplir con los tiempos de respuesta requeridos. A diferencia de otros informes, en este, no s\u00f3lo predicen la carga de trabajo, sino tambi\u00e9n la demanda futura de recursos. Su modelo proporciona detalles adicionales al proveedor de la nube sobre cu\u00e1ndo y c\u00f3mo escalar o desescalar los contenedores. El modelo considera escenarios realistas, con m\u00faltiples m\u00e1quinas virtuales en una m\u00e1quina f\u00edsica y varios contenedores en una VM. Las solicitudes de tareas se env\u00edan a trav\u00e9s de un equilibrador de carga y se almacenan en un b\u00fafer conectado a la cola de equilibrio para la asignaci\u00f3n de recursos. Se establece un Acuerdo de Nivel de Servicio (SLA) entre los usuarios y el proveedor de servicios en la nube (CSP) para determinar la Calidad de Servicio (QoS).En resumen, las solicitudes de tareas se distribuyen uniformemente entre las m\u00e1quinas f\u00edsicas en el centro de datos en la nube (CDC). Cada tarea se asigna a un contenedor \u00fanico, y en caso de una alta demanda, se ampl\u00eda el CDC agregando contenedores seg\u00fan sea necesario.</p> <p>Para modelar el Centro de Datos en la Nube (CDC), se emplea una red de colas abierta de Jackson, en donde la red de colas abierta de Jackson, es una herramienta matem\u00e1tica utilizada para modelar sistemas donde las unidades fluyen continuamente a trav\u00e9s del sistema y pueden ser atendidas en un orden espec\u00edfico. En el contexto del Papers, se aplica para entender la din\u00e1mica de las solicitudes de usuarios, las m\u00e1quinas f\u00edsicas y los contenedores en el Centro de Datos en la Nube (CDC). Se asume que todas las M\u00e1quinas F\u00edsicas (PM) en el CDC son iguales, y las solicitudes de los usuarios en cada cola en cualquier nodo del CDC se manejan seg\u00fan una pol\u00edtica FIFO. Despu\u00e9s de completar el servicio, las solicitudes abandonan la PM y salen del sistema. Cada solicitud de usuario se considera una unidad m\u00ednima de c\u00f3mputo (tarea) en el CDC y solo puede ejecutarse en un contenedor. Los tiempos de servicio en el servidor del equilibrador de carga siguen una distribuci\u00f3n exponencial con un par\u00e1metro de tasa \u03bc, donde 1/\u03bc representa el tiempo medio de servicio.</p> <p>A continuaci\u00f3n se procede a visualizar el diagrama del proyecto propuesto por el Paper:</p> <p></p> <p>Algunas de las f\u00f3rmulas aplicadas para la resoluci\u00f3n de la primer parte (te\u00f3rica) del proyecto, son las siguientes:</p> <p></p> <p>Tambi\u00e9n se utilizaron las ecuacioens para calcular la probabilidad de p\u00e9rdida de cada sistema: </p> <p></p> <p>Ecuacion para calcular la utilizacion del CPU dehardware fisico:</p> <p></p> <p>Dicho esto, se realiz\u00f3 la implementaci\u00f3n por medio de JupyterNotebook: Teniendo en cuenta la siguiente tabla.</p> <p></p> <p>Se procede a analizar y a visualizar las gr\u00e1ficas obtenidas con el c\u00f3digo propuesto, cabe destacar que en el trabajo del Paper, se realiz\u00f3 una implementaci\u00f3n similar analizando las diferentes gr\u00e1ficas que a continuaci\u00f3n se detallan. En primeras instancias se procedi\u00f3 a analizar y a ir variando la cantidad de m\u00e1quinas f\u00edsicas, para corresponder con el Papers.</p> <p></p> <p>Aqu\u00ed se puede visualizar c\u00f3mo var\u00eda el tiempo de respuesta del sistema, en funci\u00f3n de c\u00f3mo van llegando las distintas peticiones, se observa un cambio de tiempo cuando ingresan 9500 peticiones/segundo, observamos que el tiempo de respuesta supera incluso los 0.4 segundos. </p> <p></p> <p></p> <p>Aqu\u00ed se observa la cantidad de abandonos que est\u00e1 teniendo el sistema al realizar las diferentes prueba, en primeras instancias la l\u00ednea azul, corresponde a 2 m\u00e1quinas f\u00edsicas, la naranja 3 m\u00e1quinas f\u00edsicas, la verde 4, la roja 5 y la violeta 6, se observa que obviamente la cantidad de paquetes perdidos disminuye al aumentar la cantidad de m\u00e1quinas f\u00edsicas, esto se debe a que el sistema puede procesar m\u00e1s informaci\u00f3n, si se producen p\u00e9rdidas al disminuir la cantidad de m\u00e1quinas f\u00edsicas, se debe a que las peticiones no se procesan y que los buffers lo limitan. A continuaci\u00f3n se fueron realizando diferentes pruebas, ya sea variando la cantidad de contenedores por MV, como la cantidad de MV.</p> <p></p> <p></p> <p>En las siguientes figuras se puede observar la capacidad de c\u00f3mo el sistema procesa los diferentes pedidos:</p> <p></p> <p></p> <p>Podemos observar como el sistema al bajar la cantidad de contenedores y de m\u00e1quinas virtuales, comienza a perder procesamiento.</p> <p></p> <p>Podemos observar que en la gr\u00e1fica 7,  a medida que aumentamos la tasa de arribo, la cantidad de tareas en el sistema  tiende a aumentar, ya que la tasa de servicio es constante, hasta un punto en donde se hace exponencial, esto se debe a que la cantidad de arribos es igual a la cantidad de tasa de servicio es decir, el rho se hace 1, por ende el sistema se hace inestable y los valores comienzan a divergir a valores muy grandes.</p> <p>En la siguiente gr\u00e1fica observamos como se estresa el CPU en funci\u00f3n de las tareas llegadas al sistema, podemos visualizar que aumentando la cantidad de m\u00e1quinas f\u00edsicas, aumenta la capacidad de procesamiento del sistema, vemos que al introducir 5 m\u00e1quinas f\u00edsicas el porcentaje de CPU llega al 100% cuando la cantidad de llegadas son de 4000 tareas/segundo, y cuando tenemos 1 m\u00e1quina f\u00edsica el sistema se satura en 1500 tareas/segundo. </p> <p></p> <p>Si nos planteamos aumentar la cantidad de m\u00e1quinas f\u00edsicas, podemos ver que la cantidad de tareas globales por segundo ser\u00e1n mucho mayor, para un mismo valor de utilizaci\u00f3n de CPU.</p> <p></p> <p>Podemos visualizar a continuaci\u00f3n c\u00f3mo responde el sistema al variar la cantidad de contenedores, es decir como disminuye el tiempo de respuesta al aumentar la cantidad de contenedores, observamos que al aumentar la cantidad de m\u00e1quinas f\u00edsicas, el tiempo se reduce considerablemente.</p> <p></p> <p>La siguiente figura ilustra c\u00f3mo el sistema puede escalar din\u00e1micamente los recursos  asignando un n\u00famero de contenedores para cumplir con un tiempo de respuesta de SLA deseado.</p> <p></p>"},{"location":"Grupo2/1-teoria/#implementacion","title":"Implementacion.","text":"<p>Una vez concluido el an\u00e1lisis del Paper, se procedi\u00f3 a la implementaci\u00f3n del proyecto. Utilizando kubernetes de minikube y administrado con kubectl, con los correspondientes archivos .yaml. Tambi\u00e9n se utiliz\u00f3 haproxy, y FastApi con uvicorn.</p> <p>La implementaci\u00f3n se hizo en el laboratorio de redes, en una red lan cableada.</p> <p>El esquema es el siguiente:</p> <p></p> <p>Se observa, por un lado, el cliente en una m\u00e1quina f\u00edsica, y por otro, el balanceador de carga con los nodos y pods en otra m\u00e1quina f\u00edsica.</p>"},{"location":"Grupo2/1-teoria/#generador-de-trafico","title":"Generador de Trafico.","text":"<p>Como primeras instancias, se procede a la realizaci\u00f3n de un script de python para poder implementar la generaci\u00f3n de tr\u00e1fico, \u00f3sea definir la cantidad de usuarios como tambi\u00e9n la tasa de arribo, cabe destacar que la tasa de arribo total, es teniendo en cuenta el producto de la cantidad de usuarios por el lambda. B\u00e1sicamente el c\u00f3digo consiste en una parte principal en donde la misma resuelve la direcci\u00f3n IP, el puerto, el n\u00famero de clientes y el valor de lambda, para  luego generar tr\u00e1fico. </p> <p>La funci\u00f3n <code>send_request</code> envia solicitudes get as\u00edncronas a la URL especificada y mide el tiempo que tarda en recibir la respuesta.</p> <p>La funci\u00f3n <code>generate_traffic</code> genera tr\u00e1fico simulando la llegada de solicitudes de manera exponencial, osea que cada usuario espera un tiempo determinado antes de enviar una solicitud y luego ejecuta la funci\u00f3n <code>send_request</code>.</p> <p>La funci\u00f3n <code>main</code> configura los par\u00e1metros del servidor, solicitando la cantidad de usuarios y el valor de lambda. Despu\u00e9s se configura el l\u00edmite de conexiones y se ejecutan las tareas de generaci\u00f3n de tr\u00e1fico para cada usuario usando asyncio.gather.</p>"},{"location":"Grupo2/1-teoria/#haproxy","title":"HAProxy.","text":"<p>HAProxy es un software de c\u00f3digo abierto que act\u00faa como un balanceador de carga y proxy TCP/HTTP, dise\u00f1ado para mejorar la disponibilidad y confiabilidad de aplicaciones o sitios web al distribuir el tr\u00e1fico entre m\u00faltiples servidores.</p> <p>Se utiliz\u00f3 el algoritmo Round Robin para distribuir equitativamente las solicitudes de clientes entre los servidores disponibles. En este m\u00e9todo, cada solicitud se asigno secuencialmente a los servidores en un ciclo continuo. Por ejemplo, si tienes tres servidores (S1, S2, S3), las solicitudes se distribuir\u00e1n en orden: S1, S2, S3, S1, S2, y as\u00ed sucesivamente. Como en nuestro caso tenemos 2 servicios diferentes declarados, haproxy redirecciona las solicitudes una y una a cada servidor.</p> <p>Aunque el Round Robin ofrece una distribuci\u00f3n justa, no considera la carga real de los servidores.</p> <p>Pasamos a desarrollar una breve explicaci\u00f3n del c\u00f3digo utilizado. Este c\u00f3digo configura HAProxy como un balanceador de carga y proxy HTTP. En la secci\u00f3n de configuraci\u00f3n global, se definen par\u00e1metros como logs, directorio ra\u00edz y opciones SSL. La configuraci\u00f3n por defecto establece opciones globales, incluyendo el modo HTTP, configuraci\u00f3n de logs y tiempos de espera. Se establecieron 2 frontends configurados: \"http_web\" escucha en el puerto 1500, manejando tr\u00e1fico HTTP y envi\u00e1ndolo al backend \"milu\". El frontend \"stats\" escucha en el puerto 1234, proporcionando estad\u00edsticas accesibles a trav\u00e9s de la <code>localhost/haproxy-stats</code>. El backend \"milu\" opera en modo HTTP, utilizando el algoritmo Round Robin para distribuir equitativamente el tr\u00e1fico entre dos servidores, \"serv1\" y \"serv2\", en puertos espec\u00edficos. Adem\u00e1s, se agrega la direcci\u00f3n IP del cliente al encabezado de la solicitud y se configuran opciones de verificaci\u00f3n para los servidores.</p>"},{"location":"Grupo2/1-teoria/#kubernetes","title":"Kubernetes.","text":"<p>Kubernetes simplifica la administraci\u00f3n de aplicaciones en entornos contenerizados, proporcionando herramientas para la gesti\u00f3n de recursos, la resiliencia y la escalabilidad. Su popularidad se debe a su capacidad para gestionar de manera eficiente la implementaci\u00f3n y operaci\u00f3n de aplicaciones distribuidas y microservicios en entornos de producci\u00f3n.</p> <p>En nuestro caso, se implement\u00f3 kubernete, m\u00e1s precisamente Minikube, en donde el mismo se realiz\u00f3 en 1 PC f\u00edsica. Para simular los respectivos clusters, se levantaron en ella, dos m\u00e1quinas virtuales, es decir 2 NODOS, esto es por una cuesti\u00f3n de recursos de hardware, ya que al estresar demasiado las PC disponibles en los laboratorio, las mismas, comenzaban a \u201ccongelarse\u201d y no era eficiente el proyecto, lo eficiente hubiera sido poder constatar y visualizar los experimentos levantando m\u00e1s de 2 nodos.</p> <p>Resulta oportuno mencionar, que se levantaron m\u00e1s de 2 PODs, por nodo, con el fin de ir comprobando cada una de las respectivas conclusiones, como por ejemplo ver si el sistema implementado tenia perdidas o no, si las hubiese habido donde, (vimos que no existen p\u00e9rdidas, y esto por el protocolo orientado a conexi\u00f3n de TCP, ya que el mismo, no permite o hace m\u00e1s eficiente y confiable el sistema), tambi\u00e9n poder observar la latencia, poder definir y observar los stack de protocolos. En el caso de Minikube se pudo interactuar dentro de las PCV como tambi\u00e9n, dentro de cada uno de los respectivos PODs.</p> <p>En esta implementacion, para seguir lo mas que se pueda el diagrama del paper, se crearon dos servicios diferentes pero que en realidad son los mismos ya que utilizan la misma aplicacion. Se declararon como dos aplicaciones diferentes, cada una asociada a un servicio y ademas se indico que cada una apunte solo a un nodo especifico. Con esto logramos que minikube no distribuya trafico entre los nodos. Como se puede observar en los archivos deployment.yaml, en la linea de \"nodeSelector\" se declara a cual nodo apuntara cada servicio.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: depokevina  #nombre de la app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: php-kevin-a\n  template:\n    metadata:\n      labels:\n        role: php-kevin-a\n    spec:\n      nodeSelector:\n        kubernetes.io/hostname: minikube\n      containers:\n      - name: php-kevin\n        image: bocha2002/servidor_exp_time:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        command: [\"/bin/sh\", \"-c\", \"uvicorn servidor_exp_time:app --host 0.0.0.0 --port 8000\"]\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"200m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n</code></pre>"},{"location":"Grupo2/2-practica/","title":"Practica","text":"<p>Para poder continuar con el proyecto abordado, se presentan a continuaci\u00f3n las diferentes pruebas que realizamos con el grupo de trabajo, en las mismas, se utilizaron diferentes software y aplicaciones para poder generar tr\u00e1fico y poder visualizarlo. </p> <p>En cuanto a las capturas obtenidas, podemos observar pruebas tanto de conexi\u00f3n como tambi\u00e9n de sniffer con el programa Wireshark, en donde en el mismo se visualizan que no hay p\u00e9rdidas de peticiones, tambi\u00e9n se analizaron los distintos stack de protocolos, y la encapsulaci\u00f3n pertinente. Podemos concluir que el software, es interesante al momento de la visualizaci\u00f3n de los arribos al sistema planteado, ya que se puede ir \u201cfiltrando\u201d los diferentes preguntas y respuestas del Cliente al Servidor, cabe destacar que tambi\u00e9n se realizaron pruebas de c\u00f3mo van variando el tiempo medio de arribo de las peticiones por medio de consola.</p> <p>En base a las pruebas realizadas pudimos ver que no tenemos perdidas, esto se debe a que el protocolo TCP tiene un control de flujo de trafico que pone en buffer a los paquetes, estos buffers se tienen tanto del lado del cliente como del servidor, y pueden ser muy grandes. En cuanto a HAProxy, tiene su buffer, pero la velocidad con la que redirecciona el trafico es muy elevada, tanto que, el tiempo con que hace estos redireccionamiento es despreciable respecto a los tiempos que manejamos en nuestro servicio. Por esta razon aproximamos todo el sistema a un modelo M/M/1.</p> <p>En una primera instancia necesitabamos observar como variaba la tasa de arribo al estresar sistema, para determinar hasta qu\u00e9 punto era capaz de resolver las peticiones seg\u00fan la cantidad de PODs presentes en cada uno de los nodos. </p> <p>Se adjuntan im\u00e1genes, en donde podemos observar como var\u00eda las tasas de arribo en funci\u00f3n de la cantidad de PODS.</p> <p></p> <p>Para 2 pods tenemos una tasa de arribo de 160 en promedio, dividido entre los 2 pods, da 80 solicitudes por pod.</p> <p>A continuaci\u00f3n se detallan las f\u00f3rmulas utilizadas para la resoluci\u00f3n de los c\u00e1lculos pertinentes:</p> <p></p> <p>Suponiendo en la teoria que cada servidor tiene una tasa de servicio de 100, y teniendo en cuenta que disponemos de 2 pods, tendriamos un mu = 200. Con este valor procedemos a calcular el tiempo teorico en funcion del lambda. En la siguiente tabla vemos el Tteorico calculado y el Treal medido en funcion de lambda:</p> <p></p> <p>Podemos ver que para un mismo valor de lambda se tiene un tiempo real del sistema mayor que el teorico calculado. Consideramos que esto se debe a que no tenemos en cuenta el tiempo que le lleva a las aplicaciones de python ejecutar una tarea, asi como tambien el tiempo de retardo que ocaciona el control de flujo de TCP al poner a las solicitudes en bufer.</p> <p>En la siguiente figura podremos visualizar los mismos datos de la tabla:</p> <p></p> <p>Ahora procedemos a calcular el valor teorico del rho para compararlo a un rho mas cercano a lo real. Para el rho teorico utilizamos un mu = 200 y fuimos variando lambda. Para el rho \"real\" utilizamos el valor de mu = 160 que determinamos mas arriba.</p> <p>Estos datos los plasmamos en la tabla de abajo:</p> <p></p> <p>Observamos, en la siguiente figura, que para un mismo valor de lambda tenemos una tasa de utilizacion real de todo el sistema mayor a la tasa teorica, o rho teorico. Esto es debido a que la tasa de servicio real que promediamos (160) es menor que la teorica.  </p> <p></p> <p>Otro analisis que nos parecio pertinente destacar es analizar el redireccionamiento roundrobin de HAProxy, ya que al declarar los servicios como nodeport y crear dos servicios, uno asociado a cada nodo, el encargado de hacer el balanceo de carga sera haproxy. Es decir, se debe ver que la mitad de los paquetes son enviados a un nodo y la otra mitad al otro nodo.</p> <p>Desde el cliente:</p> <p></p> <p></p> <p>Se puede ver que hay un solo socket en el cliente y se envian 8 paquetes por ese socket.</p> <p>Desde el lado del servidor, en su interfaz fisica enp4s0:</p> <p></p> <p>Si se observan los sokets en la seccion de TCP se vera que tambien utiliza un solo socket para recibir los paquetes.</p> <p>Ahora si snifeamos en la interfaz virtual del servidor, vboxnet0, podremos ver las comunicaciones entre la interfaz con direccion ip <code>.59.1</code> y los nodos. Algo que se diferencia rapidamente es que del lado del backend se deben tener como minimo 2 sockets, y es lo que observamos en las siguiente imagenes:</p> <p></p> <p></p>"},{"location":"Grupo2/3-concluciones/","title":"Conclusiones","text":"<p>Como conclusiones generales, podemos decir que se establecieron con \u00e9xito ambas partes de elaboraci\u00f3n, tanto para la parte del Paper, como tambi\u00e9n la parte f\u00edsica. En cuanto al Paper, pudimos sacar diferentes conclusiones detalladas con anterioridad, pero la idea general de c\u00f3mo var\u00edan los sistemas al aumentar la cantidad y calidad de procesamiento fue aplicada con \u00e9xito, m\u00e1s all\u00e1 de las diferencias con el mismo, se pudo constatar c\u00f3mo afecta la cantidad de tr\u00e1fico seg\u00fan el tipo de sistema que tengamos presentes al variar la cantidad de m\u00e1quinas f\u00edsicas, virtuales o incluso la cantidad de contenedores presentes en el sistema. Tambi\u00e9n se pudo visualizar las p\u00e9rdidas producidas al disminuir la cantidad de m\u00e1quinas f\u00edsicas, esto es por el tama\u00f1o del buffer preestablecido, en el sistema de colas M/M/1/C como tambi\u00e9n en el de pura perdida M/M/k/k. Se lleg\u00f3 a la conclusi\u00f3n de que se necesita tener presente siempre el tri\u00e1ngulo detallado con anterioridad de Capacidad del Sistema, Calidad, y Tr\u00e1fico Ofrecido, ya que al variar, por ejemplo la cantidad de tr\u00e1fico o el sistema, puede llegar a haber un cambio significativo en nuestra calidad.</p> <p>En cuanto a la parte pr\u00e1ctica, podemos decir, que no se lleg\u00f3 exactamente igual a las pruebas del Paper, consideramos que esto se debe a que obviamente existen ciertas latencias en los medios f\u00edsicos, como tambi\u00e9n la implementaci\u00f3n del protocolo de transporte TCP, en donde concluimos que por causa del mismo, no existieron p\u00e9rdidas. Se pudo llegar a conocer ampliamente la capacidad que tienen sistemas como kubernetes y el potencial para poder desplegar Clusters sin la necesidad de recursos tan avanzados, pudimos concluir que es una herramienta muy eficiente. Tambi\u00e9n se desarrollaron distintos c\u00f3digos como se mencion\u00f3 con anterioridad, de Python, tanto para el cliente como para el servidor, pudiendo observar e investigar las diferentes funciones que se utilizan para poder abordar diferentes necesidades para el desarrollo de Software. Se implement\u00f3 el uso de la herramienta FastApi, con la cual se realizaron los servidores para poder ejecutarlo, esto llev\u00f3 a tener que investigar sobre la misma.</p> <p>A continuaci\u00f3n realizamos un an\u00e1lisis de la capacidad,calidad y tr\u00e1fico ofrecido.</p> <p></p> <p>En la siguiente grafica vemos la capacidad del sistema en funcion del lambda, teniendo un trafico fijo:</p> <p></p> <p>Se puede ver que la calidad del servicio crece linealmente si aumentamos la capacidad del sistema. Esto va a estar limitado por el hardware de nuestro cluster, va a llegar un punto donde no se podran levantar mas pods y la capacidad del sistema se mantendria constante.</p> <p></p> <p>Ahora veremos la calidad del servicio en funcion de la carga de trafico.</p> <p></p> <p>Al tener una capacidad de sistema fija, la calidad del sistema disminuira al aumentar la tasa de arribo.</p> <p></p> <p>Por ultimo tenemos la grafica de la calidad del servicio en funcion de la capacidad del sistema, dejando un trafico ofrecido fijo.</p> <p></p> <p>Si dejamos el trafico que llega fijo y aumentamos la cantidad de pods podemos notar que el tiempo medio de respuesta disminuye. Como podemos ver en la grafica de abajo:</p> <p></p> <p>Concluimos que este trabajo nos fue de mucha utilidad para comprender qu\u00e9 es, c\u00f3mo funciona y c\u00f3mo crear un cluster con kubernetes. Ademas de llevar los conocimientos adquiridos en la teoria a la practica.</p> <p>Tambien pudimos ver que es algo que se utiliza mucho en la actualidad y que hay una gran cantidad de herramientas diferentes disponibles para llevar a cabo estas tareas. </p>"},{"location":"Grupo3/Conclusion/","title":"Conclusion","text":""},{"location":"Grupo3/Conclusion/#conclusion-de-la-experimentacion","title":"CONCLUSION DE LA EXPERIMENTACION","text":"<p>Durante el desarrollo de este proyecto, buscamos la  investigacion e implementacion de tecnolog\u00edas fundamentales como Kubernetes, HAProxy y Docker. Enfrentamos varios desaf\u00edos, algunos que pudimos abordar de manera efectiva y otros no debido a la falta de tiempo para terminar el proyecto.</p> <p>Uno de los problemas mas relevantes que pudimos salvar fue de infrastructura.  Al descubrir un puente entre los pods, esto se resolvio mediante la creaci\u00f3n de servicios distintos y su asignaci\u00f3n a nodos espec\u00edficos.</p> <p>En cuento a los que no pudimos solventar surgi\u00f3 en el \u00e1mbito del balanceo de carga. Inicialmente, la configuraci\u00f3n \"round robin\" en HAProxy funcionaba de manera eficiente, garantizando la distribuci\u00f3n equitativa de solicitudes entre los nodos. La hipotesis que sostenemos es que fue debido a la introducci\u00f3n del generador de tr\u00e1fico con tiempos de interarribo asincr\u00f3nicos lo cual gener\u00f3 complicaciones en la carga distribuida generando que no se respete su configuracion asignada, esto se debe a que en situaciones asincr\u00f3nicas, exist\u00eda la posibilidad de que algunas solicitudes se generaran y enviaran m\u00e1s r\u00e1pidamente que otras. Si alguna solicitud se bloque\u00f3 o experiment\u00f3 retrasos significativos, esto pudo haber afectado el equilibrio de carga. Este problema podria haberse solventarse cambiando aspectos de configuracion de haproxy o cambiando ese aspecto del codigo del generador de trafico, sin embargo debido a la falta de tiempo eso no pudo lograrse.</p> <p>Finalmente, algunos aspectos que pudimos observar fueron los siguientes: Los sockets tcp que se generaban, no iban directamente de cliente a servidor, sino que se creaba un socket desde cliente a haproxy y luego desde haproxy a servidor. Ademas ellos tenian una conexion persistente ya que se cerraban una vez cortada la ejecucion del programa que generaba trafico. Otro aspecto fundamental, fue que no encontramos perdidas de solicitudes, ya que en diversas pruebas las solicitudes enviadas siempre  era igual a las solicitudes que recibiamos. Por lo tanto descartamos que se pudiera modelar ese esquema como \"MMKK\" o a pura perdida. La hipotesis que sostenemos respecto a esto, es que quien esta evitando esto es TCP ya que tiene mecanismos como ventaneo o control de flujo que permiten evitar la perdida de solicitudes. Por ultimo con respecto a los tiempos que analizamos,  por un lado los resultados variaban en un principio en funcion de los recursos de la computadora donde generabamos la infraestructura, por otro lado pese a ser distintos se aproximan bastante a lo que nos devolvia la aplicacion. Ademas pudimos observar claramente la divergencia teorica cuando tomabamos una carga de trabajo \u03c1 mayor que uno, lo cual implicaba saturacion. Para culminar con esto, queremos agregar que pese a todas las dificultades del trabajo,  creemos que fue muy importante como parte de nuestro desarrollo como ingenieros aprender a armar un proyecto y enfrentarnos con todo los desafios que supone. </p>"},{"location":"Grupo3/Parcial_1/","title":"Graficas primer Parcial - Conclusiones","text":"<p>Para realizar todas las graficas se utilizaron los siguientes parametros de la Tabla 1, propuestos por los autores del paper en sus experimentos.</p> <p> Tabla 1: Parametros de simulacion con sus valores correspondientes   </p>"},{"location":"Grupo3/Parcial_1/#primera-grafica","title":"Primera Grafica","text":"<p>Para este primer experimento, se evaluaron los distintos tiempos de respuesta que presenta el sistema en conjunto, a partir de variaciones en la cantidad de maquinas fisicas, lo que produjo variaciones en la cantidad total de maquinas virtuales (VM). En esta grafica, podemos observar que a medida que se aumenta la tasa de arribo en el sistema, se produce un aumento en el tiempo de respuesta de este. Ademas, se observa que no hay cambios importantes en los tiempos cuando se maneja una tasa de arribo global comprendida entre los 5500 tareas/seg y las 9500 tareas/seg. Sin embargo, cuando nos aproximamos a las 10000 tareas/seg podemos ver que en los 4 casos representados, se produce un aumento de manera exponencial en los tiempos de respuestas. </p>"},{"location":"Grupo3/Parcial_1/#segunda-grafica","title":"Segunda Grafica","text":"<p>En esta grafica, se busca representar la cantidad de tareas perdidas en el sistema, a medida que se aumenta la tasa de arribo global de tareas. Aqui se puede observar que la cantidad de tareas perdidas aumenta a medida que se aumenta la tasa de arribo global. Se puede destacar que a medida que se aumenta la cantidad de maquinas virtuales, se disminuye la cantidad de tareas perdidas.</p>"},{"location":"Grupo3/Parcial_1/#tercera-grafica","title":"Tercera Grafica","text":"<p>En la grafica numero 3, se representa el rendimiento del sistema de los equipos utilizados por los autores del paper a medida que se aumenta la tasa de arribo global. Se puede observar que a medida que aumenta la cantidad de contenedores en el sistema (producto del aumento de maquinas fisicas, y por ende, de maquinas virtuales) se produce un aumento en el rendimiento total del sistema. Ademas, se puede observar que a partir de ciertos valores de tasa de arribo global, los valores de las graficas permanecen estables, por lo que en esos casos no se pueden procesar mas tareas/seg.</p>"},{"location":"Grupo3/Parcial_1/#cuarta-grafica","title":"Cuarta Grafica","text":"<p>En la cuarta grafica, se representa el numero total de tareas en el sistema, a medida que se varia el numero total de contenedores. Aqui se puede observar que a medida que se usa un numero alto de contenedores, y si no se superan las 5000 tareas/seg, se puede obtener un numero alto de tareas dentro del sistema. En el caso de que se superen las 5000 tareas/seg en la tasa de arribo global, el cuarto caso (50 VM - 18 C) es el que puede alojar mayor cantidad de tareas totales. </p>"},{"location":"Grupo3/Parcial_1/#quinta-grafica","title":"Quinta Grafica","text":"<p>En esta ultima grafica, se representa el porcentaje de utilizacion del CPU de los equipos de los autores del Paper, a medida que se aumenta la tasa de arribo global en el sistema. En esta grafica se observa que para los primeros casos graficados, se llega al 100% de utilizacion del CPU a menor cantidad de tareas/seg, a diferencia de los ultimos, para los cuales es necesaria una mayor cantidad de tareas para lograr el mismo resultado. Por lo tanto se puede concluir que aumentar el numero de maquinas virtuales, y por ende de contenedores, produce una disminucion en la carga que tiene los CPUs utilizados.</p>"},{"location":"Grupo3/Pruebasfi/","title":"RESULTADOS DEL EXPERIMENTO","text":""},{"location":"Grupo3/Pruebasfi/#tasa-de-servicio","title":"Tasa de servicio \" \u03bc\"","text":"<p>Para realizar el experimento, se fue variando el numero de contenedores en el sistema y se obtuvo el tope de rps en cada caso. El resultado fue el siguente:</p> <p>Fig.1 rps para sistema con 2 contenedores</p> <p></p> <p>Fig.2 rps para sistema con 4 contenedores</p> <p></p> <p>Fig.3 rps para sistema con 6 contenedores</p> <p></p> <p>Fig.4 rps para sistema con 8 contenedores</p> <p></p> <p>En funcion de las rps vistas anteriormente, pudimos obtener el \"\u03bces\"  (tasa de servicio del sistema estimado) y el \"\u03bcp\" (tasa de servicio estimado de cada contenedor) para cada sistema.</p> cantidad contenedores en el sistema \u03bces \u03bcp 2 155 77.5 4 355 88.75 6 600 100 8 790 98.75"},{"location":"Grupo3/Pruebasfi/#tiempo-medio-de-respuesta-del-sistema-t","title":"Tiempo medio de respuesta del sistema \"T\"","text":"<p>Para esta prueba utilizamos un sistema de 2 maquinas virtuales, cada una con un contenedor donde se ubicaba un servidor.</p> <p>El objetivo de esta prueba fue contrastar los tiempos de respuesta obtenidos del experimento, con los tiempos de respuesta estimados.</p>"},{"location":"Grupo3/Pruebasfi/#formulas-utilizadas","title":"Formulas utilizadas","text":"<p>tasa de arribo del sistema</p> \\[ \\lambda =\\mu es*\\rho \\] <p>\u03bb : tasa de arribo del sistema </p> <p>tiempo medio de servicio en el sistema</p> \\[ T= \\frac{1}{\\mu es-\\lambda} \\] <p>Te: tiempo estimado</p> <p>Tr: tiempo real</p> <p>tasas de servicio</p> \\[ \\mu es=155=77.5*c=77.5*2 \\] <p>\u03bces: tasa estimada de servicio del sistema</p> <p>c: cantidad de contenedores del sistema</p> <p>Para cada uno de los casos que siguen, se establecio una carga y se determino el \u03bb del sistema para cumplir con ella.</p> <p>Caso 1</p> <p>Considerando una carga de \u03c1=0.129</p> \\[ \\lambda =\\mu es*\\rho=155*0.129=20 \\] <p>El tiempo de respuesta estimado fue el siguiente:</p> \\[ Te= \\frac{1}{\\mu  e-\\lambda}=\\frac{1}{\\ 155 -\\ 20}=0.007 [s] \\] <p>El resultado obtenido fue el siguiente: </p> <p>Fig.5 Tiempo de respuesta para \u03bb=20</p> <p></p> <p>Caso 2</p> <p>Considerando una carga de \u03c1=0.709</p> \\[ \\lambda =\\mu es*\\rho=155*0.709=110 \\] <p>El tiempo de respuesta estimado fue el siguiente:</p> \\[ Te= \\frac{1}{\\mu  e-\\lambda}=\\frac{1}{\\ 155 -\\ 110}=0.02 [s] \\] <p>El resultado obtenido fue el siguiente: </p> <p>Fig.6 Tiempo de respuesta para \u03bb=110</p> <p></p> <p>Caso 3</p> <p>Considerando una carga de \u03c1=0.387</p> \\[ \\lambda =\\mu es*\\rho=155*0.387=60 \\] <p>El tiempo de respuesta estimado fue el siguiente:</p> \\[ Te= \\frac{1}{\\mu  e-\\lambda}=\\frac{1}{\\ 155 -\\ 60}=0.01 [s] \\] <p>El resultado obtenido fue el siguiente: </p> <p>Fig.7 Tiempo de respuesta para \u03bb=60</p> <p></p>"},{"location":"Grupo3/Pruebasfi/#tabla-de-resultados","title":"Tabla de resultados","text":"\u03c1 \u03bb Te Tr 0.129 20 0.007 0.0152 0.25 40 0.008 0.0156 0.38 60 0.01 0.0162 0.7096 110 0.02 0.0238 0.87 135 0.05 0.061 0.96 150 0.2 0.28 0.99 155 1 1.06"},{"location":"Grupo3/Pruebasfi/#graficas-obtenidas","title":"Graficas obtenidas","text":"<p>Fig.8 Graficas de resultados obtenidos </p>"},{"location":"Grupo3/Pruebasfi/#caso-1","title":"caso \u03c1 &gt; 1","text":"<p>En este caso, pudimos observar lo que pasaba con el experimento cuando habia sobrecarga en el sistema, lo cual  implicaba que se llegaba el \"punto de saturacion\". Como pudimos observar, los tiempos de respuesta divergian. </p> <p>Fig.9 Rho mayor que 1</p> <p></p>"},{"location":"Grupo3/Pruebasfi/#perdida-de-solicitudes","title":"Perdida de solicitudes","text":"<p>El objetivo de esta prueba fue determinar si habia perdida de solicitudes en el sistema, comparando las sps con respecto a las rps</p> <p>Fig.10 Conteo de paquetes en wireshark</p> <p></p>"},{"location":"Grupo3/Pruebasfi/#balanceo-de-carga","title":"Balanceo de carga","text":"<p>Esta prueba se realizo con el fin de verificar si el balanceo se estaba haciendo de forma correcta</p> <p>Fig.11 resultado de balanceo de haproxy en wireshark</p> <p></p>"},{"location":"Grupo3/modeloteorico/","title":"MODELO TEORICO","text":"<p>Nuestro paper de referencia para hacer este trabajo propon\u00eda un modelo para el escalamiento din\u00e1mico basado en la teor\u00eda de colas para escalar recursos virtuales de contenedores y satisfacer los acuerdos de nivel de servicio (SLA) del cliente manteniendo los costos de escalamiento bajos. El objetivo era mejorar la utilizaci\u00f3n de los recursos inform\u00e1ticos virtuales y satisfacer las limitaciones del SLA en t\u00e9rminos de utilizaci\u00f3n de la CPU, tiempo de respuesta del sistema, tasa de abandono del sistema, n\u00famero de tareas del sistema y rendimiento del sistema.</p>"},{"location":"Grupo3/modeloteorico/#implementacion","title":"IMPLEMENTACION","text":"<p>En funci\u00f3n del mismo, generamos el siguiente modelo te\u00f3rico inicial mostrado en la Fig.1</p> <p>Fig.1 diagrama de red </p>"},{"location":"Grupo3/modeloteorico/#tecnologias-requeridas","title":"Tecnolog\u00edas requeridas","text":"<p>Para realizar estos experimentos, requerimos:</p> <ul> <li> <p>Cliente.py: este fue un programa en Python que nos brind\u00f3 la posibilidad de generar tr\u00e1fico siguiendo tiempos de interarribo con distribuci\u00f3n exponencial.</p> </li> <li> <p>Servidor.py: este fue un programa en Python que, junto con la utilizaci\u00f3n de FastAPI, nos permiti\u00f3 generar un servidor cuyo tiempo de servicio era gobernado por una constante agregada en el programa.</p> </li> <li> <p>Docker: fue una plataforma que nos brind\u00f3 la posibilidad del uso de entornos ligeros y port\u00e1tiles que incluyeron todo lo necesario para que la aplicaci\u00f3n del servidor pudiera ejecutarse correctamente.</p> </li> <li> <p>Kubernetes: Kubernetes fue clave para este trabajo, actu\u00f3 como una plataforma de orquestaci\u00f3n que simplific\u00f3 la administraci\u00f3n, escalabilidad y despliegue de la aplicaci\u00f3n del servidor. Para poder realizar el networking, hicimos uso de deployments y servicios. Los deployments fueron manifiestos que nos permitieron generar un conjunto de pods, seleccionar qu\u00e9 contenedor iban a albergar (en este caso, nuestro servidor) y qu\u00e9 nodo o m\u00e1quina virtual iba a estar vinculado. Por otro lado, nos permiti\u00f3 crear servicios que posibilitaron la separaci\u00f3n y aislaci\u00f3n de los dos conjuntos de nodos y adem\u00e1s el uso de \"Node port\" que nos brindaba un puerto externo para poder acceder a nuestros pods desde afuera.</p> </li> <li> <p>HaProxy: es un balanceador de carga, se dice que es un \"proxy\" ya que actu\u00f3 como intermediario entre los clientes y los servidores. Cuando un cliente realizaba una solicitud, HAProxy la recib\u00eda y decid\u00eda a qu\u00e9 servidor reenviarla seg\u00fan las reglas de balanceo de carga configuradas. En nuestro caso, lo utilizamos para balancear entre nuestros dos nodos o m\u00e1quinas virtuales, todo el tr\u00e1fico que proven\u00eda de Cliente.py.</p> </li> </ul>"},{"location":"Grupo3/modeloteorico/#ajuste-del-modelo-teorico-respecto-a-investigacion-pruebas-y-aspectos-que-podiamos-gobernar","title":"Ajuste del modelo te\u00f3rico respecto a investigaci\u00f3n, pruebas y aspectos que pod\u00edamos gobernar","text":""},{"location":"Grupo3/modeloteorico/#modelo-mm1","title":"Modelo MM1","text":"<p>La primera M correspond\u00eda al proceso de arribo, esta se pod\u00eda gobernar ya que el tiempo de interarribo de Cliente.py se gobernaba mediante el \u03bb cuyo valor depend\u00eda de la decisi\u00f3n del usuario que ejecutaba el programa.</p> <p>La segunda M correspond\u00eda al proceso de servicio, esto depend\u00eda de HaProxy y no se pod\u00eda gobernar.</p> <p>El 1 implicaba que el buffer era infinito, esto en la realidad no exist\u00eda, pero ya que ten\u00eda un buffer de 16 Mbytes, para el fin de este trabajo se consideraba infinito.</p>"},{"location":"Grupo3/modeloteorico/#modelo-mm1c","title":"Modelo MM1C","text":"<p>La primera M del proceso de arribo no pod\u00eda gobernarse porque depend\u00eda de HaProxy, como mencionamos antes.</p> <p>La segunda M del proceso de servicio pod\u00eda gobernarse ya que el \u03bb era el del sistema dividido por el n\u00famero de m\u00e1quinas virtuales o nodos.</p> <p>1 era la cantidad de servidores y C era el tama\u00f1o del nodo, el cual ya que Kubernetes no gestionaba tareas, era 1.</p> <p>Dado que en las pruebas, como se detallar\u00e1 luego, no se encontr\u00f3 p\u00e9rdida de solicitudes, no pod\u00eda ser MMKK.</p>"},{"location":"Grupo3/modeloteorico/#modelo-mmkk","title":"Modelo MMKK","text":"<p>La primera M del proceso de arribo se gobernaba, ya que era el \u03bb mencionado anteriormente en los nodos dividido la cantidad de pods.</p> <p>La segunda M del proceso del servicio tambi\u00e9n pod\u00eda gobernarse, debido a que pod\u00edamos determinar la tasa de servicio de los pods en el Servidor.py que configuramos.</p> <p>Dado que no se encontr\u00f3 p\u00e9rdidas como mencionamos antes, tampoco se pod\u00eda considerar un modelo a pura p\u00e9rdida KK.</p> <p>Finalmente, teniendo las consideraciones anteriores, propusimos que todo el sistema pod\u00eda modelarse como MM1.</p>"}]}